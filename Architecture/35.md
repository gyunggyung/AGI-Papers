---
id: 35
category: Architecture
title: 두 모델 모두 30B의 몸집을 가졌지만, 실제로는 3B만큼만 힘을 쓰는(A3B) 고효율 MoE 모델입니다. 하지만 뜯어보니 설계 철학이 완전히 정반대였습니다.
---
두 모델 모두 30B의 몸집을 가졌지만, 실제로는 3B만큼만 힘을 쓰는(A3B) 고효율 MoE 모델입니다. 하지만 뜯어보니 설계 철학이 완전히 정반대였습니다.

핵심 관전 포인트 4가지입니다.


1. 아키텍처: DeepSeek의 유전자 vs 하이브리드 돌연변이

두 모델은 엔진부터 다릅니다.

 - GLM-4.7-Flash (47 Layers): 뜯어보니 이 녀석, "DeepSeek-V3의 리틀 브라더"입니다. 모든 레이어가 Attention 기반이지만, DeepSeek의 전매특허인 MLA(Multi-Head Latent Attention) 기술을 차용했습니다. 뼈대(Hidden Size)를 2048로 얇게 줄이고, 층수를 47층으로 쌓아 지능을 높인 순수 두뇌파입니다.

 - Nemotron-3-Nano (52 Layers): 엔비디아는 하이브리드 도박을 걸었습니다. 총 52개 레이어 중 단 6개만 Attention을 쓰고, 나머지는 Mamba2(SSM)로 채웠습니다. Attention이 무거우니 꼭 필요한 기억만 남기고 나머지는 가벼운 Mamba에게 맡긴 겁니다.


2. 지능(Reasoning) vs 실무(Coding & Tools)

벤치마크 숫자 싸움이 치열합니다. (베이스라인인 Qwen3, GPT-OSS와 비교)

 - 순수 지능 (Math): GLM의 승리입니다. AIME 25(No Tools) 기준 91.6점으로 Nemotron(89.1점)을 앞섭니다. MLA 구조가 복잡한 추론에는 더 유리해 보입니다.
 - 실전 도구 (Tools): 하지만 도구를 쥐여주면 이야기가 달라집니다. Nemotron은 도구 사용 시 AIME 점수가 99.2점까지 치솟습니다.
 - 코딩 (LiveCodeBench): 최신 코딩 트렌드 반영 평가에서는 Nemotron(68.3)이 GLM(64.0)을 이겼습니다.

⚠️ 주의 (SWE-bench): GLM이 59.2점으로 Nemotron(38.8점)을 압살하는 것처럼 보입니다. 하지만 함정이 있습니다. GLM은 자체 최적화(Verified) 환경이고, Nemotron은 범용(OpenHands) 환경입니다. 시험장은 GLM에게 훨씬 유리했습니다.


3. 지구력과 속도: 100만 토큰의 벽

여기서 엔비디아의 Mamba 설계가 빛을 발합니다.

 - Context Length: GLM은 200K 수준이지만, Nemotron은 무려 1M(100만) 토큰을 처리합니다. Mamba 아키텍처 특성상 길이가 길어져도 메모리가 터지지 않습니다.
 - Speed: 논문에 따르면 Nemotron은 동급 Qwen3 모델 대비 추론 속도가 최대 3.3배 빠릅니다.


4. 투명성과 라이선스 (The Catch)

가장 중요한 현실적인 문제입니다.

 - 벤치마크 투명성: 엔비디아는 수십 개의 벤치마크 결과를 빽빽하게 공개한 반면, GLM은 자신에게 유리한 소수의 결과만 공개했습니다. 비교 데이터가 적다는 건 GLM에게 유리한 운동장이라는 뜻입니다.
 - 라이선스: 반면 라이선스는 GLM이 대인배입니다. 깔끔한 MIT 라이선스입니다. 엔비디아는 전용 라이선스로 이용 시 제약이 조금 더 까다롭습니다. (물론 중국 모델 특유의 검열 이슈는 논외로 칩시다.)


💡 결론: 누가 왕인가?

 - GLM-4.7-Flash: 짧고 굵게, 복잡한 수학 문제나 논리적 추론을 풀어야 한다면 이쪽이 더 똑똑합니다. (DeepSeek 맛집)
 - Nemotron-3-Nano: 책 수십 권 분량을 한 번에 넣고 분석하거나(RAG), 대규모 코딩 프로젝트를 돌린다면 Mamba 심장을 가진 이쪽이 압도적입니다.

라고 분석은 했지만, 저는 GPU가 부족해서 이 모델들을 제대로 돌려보며 비교하기가 버겁네요.

likesupport
63




