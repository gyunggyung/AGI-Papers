---
id: 111
category: RAG & Knowledge
title: SEPAL: Scalable Feature Learning
---
# SEPAL: 9천만 개 지식 그래프, V100 한 장으로 학습하기

> **"GPU Poor를 위한 승리." 거대한 지식 그래프를 다루고 싶지만 인프라 비용 때문에 망설였던 엔지니어들을 위한 구원 투수.**

## 1. 문제: 지식 그래프는 너무 크다

9천만 개의 엔티티(WikiKG90Mv2)를 임베딩하려면 수십 대의 GPU 클러스터가 필요했습니다. 하지만 **SEPAL (Scalable Feature Learning on Huge Knowledge Graphs)**은 단일 32GB V100 GPU 하나로 이 문제를 해결했습니다.

## 2. 핵심 메커니즘

### 1) Core & Propagation (우등생 과외 전략)
*   **Core**: 그래프에서 연결이 가장 많은 핵심 엔티티(Core)만 골라내어 딥러닝 모델로 학습시킵니다.
*   **Propagation**: 나머지 수천만 개의 주변부 엔티티들은 학습하지 않고, Core와의 관계(Relation)를 통해 임베딩 값을 전달(Message Passing)받습니다.

### 2) BLOCS & No Negatives
*   **BLOCS**: 그래프를 자를 때 구역을 겹치게(Overlapping) 잘라서 정보가 끊기지 않게 합니다.
*   **No Negatives**: 오답 찾기(Negative Sampling)를 과감히 버리고, **글로벌 일관성(Global Consistency)**을 맞추는 데 집중하여 학습 속도를 비약적으로 높였습니다.

## 3. 결과: 60배 가속

*   **속도**: 기존 11시간 5분 걸리던 학습을 **11분 20초** 만에 끝냈습니다.
*   **하드웨어**: 수억 원짜리 클러스터 대신 일반적인 GPU 한 장이면 충분합니다.
*   **성능**: 영화 수익 예측 등 46개 태스크에서 기존 경량화 모델들을 압도했습니다.

## 4. 결론

SEPAL은 자원이 부족하면 알고리즘으로 극복할 수 있음을 증명했습니다. 거대 지식 그래프의 민주화를 이끈 중요한 연구입니다.
