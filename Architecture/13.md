---
id: 13
category: Architecture
title: Beyond Transformers 2
---
# Beyond Transformers 2: 덩치보다 생각과 본질로

이전 글에서는 트랜스포머의 구조적 한계와 새로운 구조에 대한 이야기했습니다. 이번에는 그 한계를 뛰어넘어 트랜스포머를 진짜 잘 쓰고 있는 두 회사의 이야기를 해보려 합니다.

한 곳은 기존 GPT 구조를 극단적으로 효율화해 압도적인 퍼포먼스를 보여주는 StepFun, 다른 한 곳은 모두가 디코더(GPT)를 외칠 때 과감히 인코더(BERT)로 회귀해 혁신을 만든 Fastino입니다.

## 1. StepFun: 몸집은 가볍게, 생각은 깊게

이들은 모델의 물리적 크기보다 '생각의 양(Scaling Test-Time Compute)'이 더 중요하다는 사실을 간파했습니다.

하지만 생각을 깊게 하면 속도가 느려지기 마련입니다. StepFun은 이 문제를 두 가지 기술로 해결했습니다.

*   한 번에 3개의 토큰을 동시에 예측하는 **MTP(Multi-Token Prediction)**로 속도를 잡았습니다.
*   **PaCoRe**라는 프레임워크를 통해 여러 아이디어를 병렬로 탐색하고, 그중 핵심만 압축 및 종합해 최적의 답을 도출합니다.

결과물인 Step-3.5-Flash(196B A11B)는 수학 경시대회(HMMT) 벤치마크에서 GPT-5를 능가하는 정답률을 기록하기도 했습니다. MS 출신들이 만든 기술력 있는 중국 스타트업다운 행보입니다.

## 2. Fastino Labs: 디코더 만능주의에 던진 질문

"모든 걸 생성형(Decoder)으로 할 필요가 있을까?"

Fastino는 이 질문에 "아니요"라고 답하며 CPU 환경과 인코더 구조(BERT)에 집중했습니다.

생성형 모델은 글을 쓰는 데는 좋지만, 정보를 추출하거나 분류하는 데는 비효율적입니다. 이들은 'GLiNER'라는 모델을 통해 전통적인 개체명 인식(NER) 작업을 혁신했습니다.

놀라운 점은 효율성입니다.

GPT-4o 같은 거대 모델이 16초 걸릴 분류 작업을, GLiNER는 M단위의 파라미터로 0.2초 만에 CPU에서 끝냅니다. 성능도 뒤지지 않습니다.

물론 아직 코드를 짜거나 긴 글을 짓는 건 못합니다. 하지만 에이전트 시스템에서 "어떤 도구를 쓸지" 혹은 "어떤 모델을 쓸지" 결정하는 라우팅이나 데이터 추출 같은 업무에서는 거대 모델보다 훨씬 빠르고 정확하며 저렴합니다.

---

## 결론

무조건 모델을 키우는 'One-size-fits-all'의 시대는 저물고 있습니다.

StepFun처럼 생각의 과정을 효율화하거나, Fastino처럼 목적에 딱 맞는 가벼운 구조를 택하는 것. 이것이 앞으로의 AI가 나아갈 진짜 효율의 방향이 아닐까 합니다.

제가 생각하는 미래의 AI 시스템은 하나의 거대한 뇌가 아니라, 작지만 전문적인 여러 모델이 톱니바퀴처럼 맞물려 돌아가는 형태입니다. 제 Tiny MoA처럼 말이죠.

다시 한 번 말하지만, 저는 효율적인 온디바이스 LLM이 필요하다고 생각합니다. 보안, 통제력, 비용 측면에서 필수적입니다. 거대 테크 기업들이 API 가격을 올리거나 기술을 비공개로 전환한다면(중국은 이미 시작했습니다), 우리에겐 우리가 통제할 수 있는 모델이 있어야 합니다.
