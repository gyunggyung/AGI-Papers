---
id: 5
category: RAG
title: AIRS-Bench: a Suite of Tasks for Frontier AI Research Science Agents
---
"GPT API가 최고 아니야?"라는 상식이 깨졌습니다. 로컬 LLM이 적절한 전략과 결합했을 때, GPT-4o나 o3-mini 같은 과거 최강의 모델들을 압도했습니다. 심지어 20B(A3.6B) 짜리 작은 모델(gpt-oss-20b)이 o3-mini를 이기는 하극상까지 일어났습니다. 도대체 어떻게 된 일일까요? 

Meta FAIR 팀이 발표한 새로운 벤치마크 'AIRS-Bench'에서 나온 재미있는 결과를 5가지로 정리했습니다.

📄 제목: AIRS-Bench: a Suite of Tasks for Frontier AI Research Science Agents
👥 저자: Meta FAIR et al.

1. "생각의 나무를 키워라" (Tree Search의 마법)

이 논문의 핵심은 모델의 지능보다 '탐색하는 구조'입니다. 연구진은 에이전트에게 단순한 코딩이 아닌, 트리 탐색(Greedy Tree Search)을 시켰습니다.

 - 작동 원리: 에이전트는 [초안 작성(Draft) → 디버깅(Debug) → 성능 개선(Improve)]이라는 3가지 연산자를 사용해 수많은 솔루션 후보(Node)를 만듭니다.
 - 가지치기: 만든 코드들을 실제 검증(Validation) 셋에 돌려보고, 점수가 가장 높은 가지(Branch)만 선택해 다음 단계로 나아갑니다.

마치 바둑을 둘 때 수십 수를 앞서 계산하듯, 코드를 짜고 실행해본 뒤 가장 유망한 경로만 남기는 전략입니다. 이 생각하는 시간과 이 구조가 더해지자, 멍청했던 모델들이 천재가 되었습니다.

특히 20B짜리 작은 오픈소스 모델은 점수가 0.083에서 0.473으로 무려 5.7배나 상승했습니다. 반면 GPT-4o는 6배가 올랐음에도 0.364점에 그쳤습니다.

2. 오픈소스(Open-Weights)의 반란: 120B > GPT-4o

우리는 보통 "오픈소스는 상용 모델을 따라가는 입장"이라고 생각합니다. 하지만 이번엔 달랐습니다.

 - 1위: gpt-oss-120b (점수 0.537)
 - 2위: gpt-oss-20b (점수 0.473)
 - 3위: o3-mini (점수 0.469)
 - 5위: GPT-4o (점수 0.364)

120B A5.1B 오픈소스 모델이 GPT-4o보다 약 47% 더 높은 점수를 기록했습니다. 이는 모델의 범용 지능보다, 연구나 코딩 도메인에 얼마나 '특화(Fit)'되어 있는지가 훨씬 중요함을 시사합니다.

3. 20B 모델의 기적: "피드백만 있다면 나도 전문가"

가장 놀라운 건 20B A3.6B 모델(gpt-oss-20b)의 성과입니다. 이 작은 모델이 o3-mini를 이겼습니다.

 - gpt-oss-20b: 0.473점
 - o3-mini: 0.469점

작은 모델은 혼자서 깊은 추론을 못 합니다. 하지만 "채점 결과(Validation Score)"라는 확실한 피드백 환경이 주어지자, 끈질기게 코드를 수정하며 결국 정답을 찾아냈습니다. "똑똑한 천재 한 명(o3-mini)보다 끈질기게 검증하는 노력파(20B)"가 연구에는 더 적합했던 겁니다.

4. 모델 6종 성능 비교: "작을수록 더 크게 성장한다"

전략을 바꿨을 때(One-Shot → Greedy), 성능이 얼마나 뛰었는지 6개 모델을 비교해봤습니다. 놀랍게도 모델이 작거나 성능이 낮을수록 상승 폭이 컸습니다.

gpt-oss-120b: 0.177 → 0.537 (3.0배)
gpt-oss-20b: 0.083 → 0.473 (5.7배) 🔺
o3-mini: 0.192 → 0.469 (2.4배)
CWM(32b): 0.046 → 0.389 (8.5배) 🔺
GPT-4o: 0.060 → 0.364 (6.0배)
Devstral(24b): 0.018 → 0.217 (12.0배) 🔺


즉, 컴파일러가 모델의 지능 부족을 완벽하게 메워주는 스캐폴딩(Scaffold) 역할을 한 것입니다.

5. 인간 vs AI: 아직은 인간 승리, 하지만...

그렇다면 AI가 인간 연구자를 완전히 대체했을까요? 아직은 아닙니다.

 - 인간 SOTA 승리: 16개 과제 (80%)
 - AI 에이전트 승리: 4개 과제 (20%)

하지만 AI가 이긴 4개 과제에서 보여준 모습은 흥미롭습니다. 예를 들어, 텍스트 분류 과제에서 AI는 스스로 "RoBERTa와 DeBERTa 모델을 결합한 스태킹 앙상블(Stacked Ensemble)" 코드를 짜내어 인간 최고 기록(0.90)을 넘는 0.93점을 달성했습니다. 단순히 시키는 걸 하는 게 아니라, 더 나은 방법론을 발명한 셈입니다.

최근 나온 Claude Opus 4.6, GPT-5.3-Codex에 해당 방법론을 적용한다면 어떨지 궁금합니다. 지금은 결과가 다를까요?



"GPT-6를 기다릴 필요 없다. 지금 있는 모델로 구조(Scaffold)를 잘 짜면 된다."

우리는 그동안 더 큰 모델, 더 비싼 API에 집착했습니다. 하지만 AIRS-Bench는 20B 수준의 작은 모델이라도, 스스로 코드를 짜고 검증하고 수정하는 '루프(Loop)'만 잘 설계해주면 SOTA급 성능을 낼 수 있다는 것을 증명했습니다.

결국 미래의 AI 경쟁력은 모델의 크기가 아니라, 그 모델을 일하게 만드는 '워크플로우 설계 능력'에서 나올 것입니다.


Thank you Yoram Bachrach for sharing this wonderful work!




49




