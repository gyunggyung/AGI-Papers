---
id: 57
category: Projects
title: 연구자들이 논문을 쓸 때 가장 시간이 오래 걸리는 일이 뭘까요? 아마도 Related Work 섹션 작성과 실험일 겁니다. 실험은 자동화하기 어렵지만, Related Work 초안은 LLM이 도와줄 수 있지 않을까? 그 생각으로 LFM-Scholar를 만들었습니다.
---
연구자들이 논문을 쓸 때 가장 시간이 오래 걸리는 일이 뭘까요? 아마도 Related Work 섹션 작성과 실험일 겁니다. 실험은 자동화하기 어렵지만, Related Work 초안은 LLM이 도와줄 수 있지 않을까? 그 생각으로 LFM-Scholar를 만들었습니다.

오늘 LFM-Scholar를 공개하고 몇시간 후, Liquid AI에서 LFM2.5-1.2B 버전을 공개했습니다. 그래서 Liquid AI의 CTO Mathias Lechner님께 LFM2-2.6B-Exp와 LFM2.5-1.2B-Instruct 중 무엇이 더 좋은지를 문의했고 LFM2.5-1.2B-Instruct이 더 좋다는 답변을 받았습니다.

🔥 아마도 전세계 최초의 LFM2.5 개인 Fine-tuning 모델?

Liquid AI 회사 차원이 아닌, 개인으로서 LFM2.5-1.2B-Instruct를 Fine-tuning해서 실제 작동하는 도구를 만든 건 아마도 전세계에서 제가 처음일 것 같습니다. Mathias Lechner님의 답변에서 영감을 받아 몇시간만에 LFM2.5를 통합했습니다. (Thank you! Mathias)

📚 어떻게 학습했나?

Unsloth + LoRA로 Google Colab T4 GPU에서 학습했습니다:

- 베이스 모델: unsloth/LFM2.5-1.2B-Instruct
- LoRA 설정: r=16, alpha=16, target_modules="all-linear"
- 프롬프트 형식: ChatML (<|im_start|>system/user/assistant<|im_end|>)
- 학습 데이터: 97개 Related Work 샘플
- 학습 시간: 60 step, 약 14분 (T4 GPU)
- 최종 Loss: 3.41 → 1.62 (최저점)

사실 새벽에 LFM2-2.6B-Exp 튜닝한 것과 비슷한 방식이라서 어렵지는 않았습니다. 거기다 전부터 Maxime Labonne님께 많은 조언을 얻었어서 LFM 모델을 튜닝을 하는데 이제 어려움이 없습니다.

📊 LFM2 vs LFM2.5 비교 결과


LFM2.5가 확실히 빠르고, 놀랍게도 환각이 거의 없었습니다. 다만 LFM2-2.6B-Exp를 튜닝한 모델의 결과는 더 좋습니다.

🔮 다음 버전 계획

다음 버전에서는 Kyudan Jung님의 CiteAgent 핵심 기능을 로컬 LLM으로 구현해볼 예정입니다. CiteAgent는 Overleaf에서 작성 중인 LaTeX 논문에 자동으로 적절한 인용을 추가해주는 도구입니다. 현재는 Gemini API를 사용하지만, 이걸 로컬 LLM으로 바꾸려고 합니다.

Related Work 초안 생성 + 기존 글 인용 삽입, 두 가지를 모두 로컬에서 가능하게 하는 거죠!

또한 기존에 lfm2-1.2b-koen-mt 모델도 2.5 버전으로 다시 학습을 시킬 예정입니다.

⚠️ 현재 한계

LFM2.5는 아직 쿼리 다양성이 부족합니다. 데이터를 늘리기도 하고 다양한 실험을 해봤지만, 해당 작업에서는 LFM2-2.6B-Exp을 튜닝한 버전이 LFM2.5-1.2B-Instruct을 튜닝한 버전보다 퀄리티가 좋습니다.


🙏 감사

- Liquid AI의 LFM2.5 덕분에 28T 토큰으로 사전학습된 강력한 1.2B 모델을 쓸 수 있었습니다.
- Unsloth AI 덕분에 무료 Colab에서 15분만에 학습할 수 있었습니다.

GPU 없는 노트북에서도 731MB만으로 학술 연구 보조가 가능해졌습니다.

빠른 초안이 필요하면 --model-variant lfm2.5
높은 품질이 필요하면 --model-variant lfm2

이제 선택할 수 있습니다!

🔗 GitHub: https://lnkd.in/gF_Pa2yS
18




