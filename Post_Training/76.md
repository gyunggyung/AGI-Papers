---
id: 76
category: Post_Training
title: 최근 Liquid AI의 LFM2-1.2B 모델을 튜닝하여, 한국어-영어 번역 RL(GRPO) 학습을 돌렸습니다. 1만 개의 데이터로 LoRA 학습을 진행했고, Step 150 (약 1.9 에폭) 만에 GPU 자원 부족으로 멈춰야 했습니다.
---
최근 Liquid AI의 LFM2-1.2B 모델을 튜닝하여, 한국어-영어 번역 RL(GRPO) 학습을 돌렸습니다. 1만 개의 데이터로 LoRA 학습을 진행했고, Step 150 (약 1.9 에폭) 만에 GPU 자원 부족으로 멈춰야 했습니다.

📉 제 모델의 마지막 기록 (lfm2-1.2b-koen-mt-v5-rl-10k)
 - Step 50: +0.11 점 (v4 대비)
 - Step 100: +0.73 점
 - Step 150: +1.43 점 (CHrF++ 32.96 / BLEU 12.05)

성능은 계속해서 오르고 있었습니다. 여기서 더 돌리면 어떻게 될지 궁금했지만 멈췄습니다. 그런데 오늘 소개할 NVIDIA의 논문을 읽고 깨달음을 얻었습니다. "아, 이제 시작이었구나. 최소 10배는 더 돌렸어야 했구나."

강화학습이 모델의 한계를 뚫어주는지, 아니면 그저 아는 걸 쥐어짜는 건지에 대한 논쟁을 종결지을 논문, ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models 입니다.

이 논문은 "RL 스케일링 법칙"의 바이블과 같습니다. 핵심 인사이트를 제 경험과 엮어 4가지로 정리했습니다.


1. "Aha Moment"는 인내심 뒤에 온다 (2,000 Steps)

제 모델은 150 스텝에서 멈췄지만, 논문은 "최소 2,000 스텝 이상" 밀어붙여야 한다고 말합니다. 아래는 논문에서 나온 실험입니다.

 - Boxnet 실험: Base 모델이 수만 번을 샘플링해도 정답률이 0%인 OOD(Out-of-Distribution) 태스크가 있었습니다.

 - 결과: ProRL로 2,000 스텝 이상 길게 학습하자, 모델은 스스로 규칙을 깨우치고 정답률 100%를 달성했습니다.

 - 의미: 이는 단순히 잠재된 능력을 꺼낸 게 아니라, 모델이 미지의 솔루션 공간을 탐색(Explore)하여 새로운 전략을 학습했다는 강력한 증거입니다.


2. 약점일수록 폭발적으로 성장한다 (Diminish vs. Expansion)

이 논문에서 가장 흥미로운 점은 RL의 효과가 '모델이 원래 못하던 영역'에서 가장 컸다는 사실입니다. 

 - Diminish (감소/정체): 모델이 이미 잘하는 영역(Pass@128이 높은 태스크)은 RL을 수행해도 성능이 정체되거나, 다양성이 줄어들어 오히려 성능이 떨어지기도 합니다. 

 - Expansion (확장): 반면, Base 모델이 쩔쩔매던 영역(Pass@128이 낮은 태스크)에서는 RL이 추론 경계를 폭발적으로 확장시켰습니다. 

 - SFT의 역할: 물론 아예 모르는 건 못 배웁니다. 어려운 지식은 SFT로 먼저 가르쳐야(Warm Start) 하지만, 그 이후의 도약은 RL이 책임집니다. 1.5B 모델이 7B 모델을 이긴 비결은 바로 '약점의 보완'에 있었습니다. 


3. 뇌가 굳지 않게 "리셋"하라 (The Recipe)

하지만 무작정 RL을 길게 돌리면 모델은 한 가지 패턴만 고집하는 '엔트로피 붕괴(Entropy Collapse)'에 빠져 멍청해집니다. 연구진은 이를 'Reference Policy Reset'으로 해결했습니다.


 - KL Penalty: 최신 트렌드(DeepSeek-R1-Zero)와 달리 KL Penalty를 유지하되, 리셋을 통해 KL 제약의 기준점을 갱신해주며, 모델이 멈추지 않고 계속해서 더 멀리 탐색하도록 유도합니다.


4. 데이터 양보다 중요한 건 '깊이' (10k is Enough)

많은 분들이 "RL 데이터 수십만 개 필요하지 않나요?"라고 묻습니다. 저도 1만 개(10k)로 불안했습니다. 이 논문은 "데이터가 적어도, 깊게 파고들면 된다"는 것을 보여줍니다.

 - 데이터 효율성: 논문은 총 136k 데이터를 썼지만, '지시 따르기(Instruction Following)' 같은 특정 도메인은 불과 1만 개(10k)의 데이터만으로도 22%의 성능 향상을 이뤄냈습니다. 

 - 핵심은 스텝(Step): 데이터가 적다고 금방 끝내면 안 됩니다. 논문은 약 2,000 스텝 이상, 일반적인 학습보다 훨씬 긴 호흡(Prolonged)으로 모델을 밀어붙였을 때 비로소 성능이 폭발하는 구간(Sustained Gains)을 발견했습니다. 


💡 마치며: 번역(Translation)에 적용한다면?

보통 번역은 정답이 모호해(Non-Verifiable) 강화학습을 길게 돌리기 어렵다고 생각합니다. 하지만 저는 여기서 최근 리뷰한 DuPO(Dual Preference Optimization)와 ProRL을 결합해보려고 합니다.

DuPO의 '역번역 검증(Cycle Consistency)'을 통해 번역 과제를 '검증 가능한(Verifiable) 문제'로 정의하고, 약 1~2만 개의 정제된 데이터만 준비된다면 조건은 갖춰집니다. 여기에 ProRL의 레시피대로 충분히 긴 시간(Prolonged) 동안 모델이 스스로 시행착오를 겪게 한다면, 번역 모델도 단순히 문장을 매칭하는 수준을 넘어 "문맥의 깊은 의미를 파악하는 구조적 깨달음(Aha Point)"을 얻을 수 있을 것입니다.

제 lfm2-1.2b-koen-mt-v5-rl-10k-adapter 모델의 150 스텝 실험은 실패가 아니라 가능성의 예고편이었습니다. 데이터 1만 개는 충분합니다. 부족했던 건 오직 'GPU(Steps)' 뿐이었습니다.

추후 실험에서는 Reference Reset 기법을 적용해 뇌가 굳는 것을 막고, 모델이 스스로 최적의 번역 경로를 찾아낼 때까지 GPU가 허락하는 한 끝까지 밀어붙여 보겠습니다.

50




