---
id: 85
category: Architecture
title: Ai2가 공개한 Olmo 3는 현재 시점에서 LLM 연구자들에게 가장 중요한 교과서입니다. 다만, Olmo 3의 벤치마크 점수는 SOTA가 아닙니다. 최신 프런티어 모델들에 비해 성능은 다소 뒤처집니다. 하지만 이 모델의 진가는 결과가 아닌 과정의 투명성에 있습니다.
---
Ai2가 공개한 Olmo 3는 현재 시점에서 LLM 연구자들에게 가장 중요한 교과서입니다. 다만, Olmo 3의 벤치마크 점수는 SOTA가 아닙니다. 최신 프런티어 모델들에 비해 성능은 다소 뒤처집니다. 하지만 이 모델의 진가는 결과가 아닌 과정의 투명성에 있습니다.

대부분의 오픈 소스 모델이 가중치 파일만 던져주는 Open Weights에 머물 때, Olmo 3는 데이터, 훈련 코드, 로그, 중간 체크포인트, 그리고 비용 명세서까지 모든 것을 공개했습니다. "물고기를 주는 대신, 낚시하는 법과 낚싯대 설계도까지 공개한" Olmo 3의 핵심 포인트 4가지를 정리했습니다.

1. 투명성이라는 무기: 블랙박스를 해체하고 타임머신을 선물하다

대부분의 오픈 모델들이 "우리는 3T 토큰으로 학습했어"라고 말하고 끝낼 때, Olmo 3는 그 속살을 전부 드러냈습니다.

(1) 데이터: Dolma & Dolci 단순한 데이터 덤프가 아닙니다. 사전 학습용 Dolma 3와 사후 학습용 Dolci 데이터셋을 구축하기 위한 큐레이션 로직, 혼합 비율(Mixing Ratios), 그리고 품질 필터링 파이프라인까지 공개했습니다.

(2) 코드: Olmo-Core 학습 코드는 단순히 "돌아가는 코드"가 아닙니다. 7B 모델 기준 GPU당 초당 7,700 토큰을 처리하는 고도로 최적화된 라이브러리 olmo-core를 공개했습니다. H100 클러스터에서의 효율적인 학습을 위한 메모리 관리, 통신 최적화 노하우가 코드에 그대로 녹아 있습니다.

(3) 중간 체크포인트: 타임머신 특히 인상적인 것은 '중간 체크포인트(Step Checkpoints)'의 전면 공개입니다. 보통은 최종 모델만 공개하지만, Olmo 3는 학습 과정 전체의 스냅샷을 제공합니다. 이는 연구자들에게 타임머신을 쥐여준 것과 같습니다. 모델이 100B 토큰을 봤을 때와 500B 토큰을 봤을 때 내부 표현이 어떻게 변하는지, 복잡한 추론(CoT) 능력이 어느 시점에 발생하는지를 시간 축을 따라 해부할 수 있게 된 것입니다.


2. 생각하는 모델의 레시피: TIS로 잡은 수치적 안정성

Olmo 3는 SFT(지도학습) → DPO(선호도 정렬) → RLVR(검증 가능한 강화학습)로 이어지는 정석적인 파이프라인을 따릅니다.

여기서 주목할 핵심 기술은 '절단된 중요도 샘플링(Truncated Importance Sampling, TIS)'입니다.


Olmo 3는 TIS를 통해 이 두 엔진 사이의 '사투리'를 통역하고 보정했습니다. 학습 엔진의 확률과 추론 엔진의 확률 사이의 비율을 계산하여 손실 함수에 가중치로 곱해주되, 이 비율이 너무 커지거나 작아지면 과감히 잘라내어(Truncate) 수치적 안정성을 강제했습니다. 이는 이기종 분산 학습 환경에서 RL을 안정적으로 돌리기 위한 필수적인 엔지니어링 안전핀입니다.


3. 인프라의 마법: 15일 걸릴 학습을 6일로 단축하다 (OlmoRL)

Reasoning 모델 학습의 가장 큰 병목은 생성입니다. 학습은 순식간이지만, 모델이 긴 사고 과정을 생성하는 건 하염없이 오래 걸립니다. 실제로 GPU가 데이터를 기다리며 노는 시간이 75%에 달하기도 합니다.

Olmo 3 팀은 'OlmoRL'이라는 전용 인프라로 이를 해결했습니다.

(1) 완전 비동기식 구조 (Fully Asynchronous) 기존의 Stop-and-Go 방식(생성 후 대기, 학습 후 대기)을 버리고, 생성자(Actor)와 학습자(Learner)를 완전히 분리했습니다. 


(2) In-flight Weight Update 비동기 방식의 난제는 생성자가 가진 모델이 구버전이 된다는 점입니다. OlmoRL은 추론 엔진(vLLM)을 멈추지 않고, 작동 중에 실시간으로 최신 가중치를 갈아 끼우는 'In-flight update'를 구현했습니다.


4. 비용의 투명한 감사(Audit)와 평범함의 미학

DeepSeek V3가 약 80억 원의 저비용 학습을 주장했을 때 많은 의구심이 있었습니다. Olmo 3는 서구권 인프라(H100) 기준으로 32B 모델을 바닥부터 학습시키는 데 든 비용이 약 275만 달러(약 40억 원)임을 투명하게 밝혔습니다. 

시간당 $2라는 H100 GPU 비용과 Wall-clock time을 기반으로 산출된 이 영수증은, 향후 LLM 프로젝트 예산 산정의 가장 신뢰할 수 있는 기준점이 될 것입니다.

또한, 아키텍처가 매우 평범하다는 점(Standard Dense Transformer)도 흥미롭습니다. 아키텍처가 평범하다는 것은, 성능의 차이가 오직 '데이터의 질'과 '학습 방법론'에서 온다는 것을 증명합니다. 연구자 입장에서는 변인을 통제하고 실험하기에 최적의 환경입니다.

마치며: 거인의 어깨가 넓어졌다

Olmo 3는 "우리가 얼마나 대단한지 봐라"라고 말하는 모델이 아닙니다. "너희도 이렇게 하면 만들 수 있어"라고 말하며 설계도를 건네는 모델입니다.

아키텍처는 평범할지라도, 그 평범함을 비범한 성능으로 끌어올린 '데이터 큐레이션'과 'RL 엔지니어링'의 디테일은 실전에서 깨져본 사람만이 줄 수 있는 귀한 지식입니다. LLM을 단순히 사용하는 단계를 넘어, 그 원리를 이해하고 직접 만들어보고 싶은 모든 엔지니어에게 Olmo 3는 최고의 참고서가 될 것입니다.

39

love
마음에 쏙듬



