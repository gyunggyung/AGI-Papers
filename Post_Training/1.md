---
id: 1
category: Post_Training
title: iGRPO: Self-Feedback-Driven LLM Reasoning
---
GRPO를 항상 이기는 자가개선 RL, iGRPO

"수학 문제를 풀 때, 펜을 잡자마자 답안지에 정답을 적어내는 학생과 연습장에 먼저 풀어보고 가장 잘 쓴 풀이를 골라 답안지에 옮겨 적는 학생. 과연 누가 더 점수가 좋을까요?"

우리는 그동안 LLM에게 전자를 강요했습니다. "한 번에 정답을 내놓으라"고 닥달했죠. 하지만 NVIDIA 연구진이 발표한 iGRPO는 후자의 방식을 택했습니다. 모델이 스스로 만든 '초안(Draft)'을 보고 다시 배우게 했더니, 모든 지표에서 성능이 크게 올랐습니다.

📄 제목: iGRPO: Self-Feedback-Driven LLM Reasoning
👥 저자: Ali Hatamizadeh et al. (NVIDIA)

단순히 "좋아졌다"가 아니라, "얼마나, 어떻게, 왜" 좋아졌는지 4가지 핵심으로 뜯어봤습니다.

1. GRPO vs iGRPO: "전승무패(全勝無敗)"

논문을 보며 가장 놀라웠던 점은 일관성입니다. 보통 특정 벤치마크에서는 점수가 떨어지기 마련인데, iGRPO는 체급과 종목을 가리지 않고 모든 부분에서 GRPO를 압도했습니다. (동일 샘플링 예산 기준)

 - 일반 모델 (Nemotron-8B): GRPO 대비 +3.96%p 상승 (41.08% -> 45.04%)
 - SOTA 모델 (DeepSeek-R1-Distill): GRPO 대비 +1.58%p 상승 (68.29% -> 69.87%)
 - 14B 모델: GRPO 대비 +1.73%p 상승 (71.29% -> 73.02%)

"고작 1.5%?"라고 하실 수 있습니다. 하지만 이미 90점대인 전교 1등(SOTA)이 점수를 더 올리는 건, 50점짜리가 80점 되는 것보다 훨씬 어렵습니다. iGRPO는 마른 수건을 쥐어짜듯, 이미 강력한 모델의 성능을 한계까지 밀어붙였습니다.

2. 원리: 선생님보다 나은 '과거의 나'

기존 GRPO는 외부 정답지(Reward)만 바라보고 달립니다. 하지만 iGRPO는 두 단계로 나뉩니다.

 - 1단계(탐색): 모델이 여러 개의 초안을 작성하고, 그중 1등을 뽑습니다.
 - 2단계(학습): 그 1등 답안을 프롬프트에 끼워 넣어 다시 입력합니다. "봐봐, 네가 아까 이렇게 풀었을 때 맞았어. 이걸 참고해서 더 완벽하게 풀어봐."

이 과정이 반복되면 '부트스트래핑(Bootstrapping)'이 일어납니다. 모델이 똑똑해질수록 초안의 질이 좋아지고, 그 좋은 초안을 보고 배우니 모델이 더 가파르게 성장합니다. 엔트로피 붕괴(Entropy Collapse)를 지연시키며 더 깊게 고민하게 만드는 효과도 증명되었습니다.

3. 속도의 한계와 비용: "두 번 풀면 돈도 두 배?" -> NO

가장 중요한 질문입니다. 초안도 쓰고 정서도 하려면 시간이 2배로 들 것 같지만, NVIDIA 팀은 영리하게 계산했습니다. 전체 생성 횟수(Budget)를 고정했기 때문입니다. (예: GRPO 16개 생성 vs iGRPO 8개 초안 + 8개 수정)

 - 메모리(VRAM): 기존 방식과 완벽하게 동일합니다. (+0.006GB 차이)
 - 시간(Speed Limit): 다만 시간은 조금 더 걸립니다. 1단계와 2단계를 순차적으로 실행해야 하기에, 병렬 처리가 가능한 GRPO보다 학습 시간이 약 13% 증가합니다.

하지만 GPU 메모리를 더 증설할 필요는 없습니다. 단지 13%의 시간만 더 투자하면, SOTA 모델의 성능을 뚫어낼 수 있습니다. AI 학습에서 보기 드문 고효율(High ROI)입니다.

4. 그림의 떡? NeMo-RL 말고 Unsloth를 기다리며

논문은 완벽해 보이지만, 현실적인 장벽은 도구입니다. NVIDIA의 NeMo-RL 프레임워크는 일반 연구자가 쓰기에 빌드부터가 고역입니다.

하지만 우리에겐 Unsloth AI가 있습니다. Daniel Han과 Michael Han (Unsloth) 형제는 최근 GRPO를 그 누구보다 빠르고 효율적으로 구현해냈습니다. iGRPO 역시 복잡한 수식이 아니라 '가장 좋은 초안을 골라 프롬프트에 넣는다'는 논리적인 프로세스이기에, Unsloth가 마음만 먹으면 금방 구현할 수 있을 것입니다.

Unsloth의 추론 최적화 기술과 iGRPO의 방법론이 만난다면, 그깟 13%의 속도 저하는 상쇄하고도 남을 겁니다. 우리들도 집에서 '자기 수정 하는 모델'을 만들 수 있는 날이 곧 오기를 기대해 봅니다.

💡 마치며: 13%의 투자가 만드는 차이

우리는 그동안 더 큰 모델, 더 많은 데이터를 찾아 헤맸지만, 정작 중요한 건 "자기가 쓴 답을 다시 검토해 보는 시간"이었을지도 모릅니다.

인간이 어려운 문제를 풀 때 검산을 하듯, AI에게도 '스스로 한 생각을 다듬을 기회'를 주는 것. 그것이 바로 iGRPO가 증명한 지능 향상의 열쇠였습니다.




3




