---
id: 6
category: Architecture
title: Generative Modeling via Drifting
---
확산 모델(Diffusion Model)이 이미지를 만들기 위해 50번, 250번씩 노이즈를 걷어내는 과정을 지켜보는 건 이제 지루합니다. "그냥 한 번에 짠 하고 만들어주면 안 돼?"라는 질문에 MIT와 하버드 연구진이 답을 내놓았습니다.

기존의 패러다임을 완전히 뒤집어, 단 한 번의 단계(1-step)만으로 SOTA(State-of-the-Art) 성능을 보여주는 'Drifting Models'입니다.

📄 제목: Generative Modeling via Drifting
👥 저자: Mingyang Deng et al. (MIT & Harvard)

확산 모델의 지루한 반복 과정을 없애고, 어떻게 압도적인 속도와 품질을 동시에 잡았는지 4가지 핵심으로 정리했습니다.

1. 생각은 '학습' 때 깊게, '실전'은 한 방에


Drifting Model은 이 고된 '진화(Evolution)'의 과정을 학습(Training) 시간으로 전부 옮겨버렸습니다.

 - 기존: 실전에서 250걸음을 걸어야 목적지 도착
 - Drifting: 학습 때 미리 지름길을 완벽히 닦아놓고, 실전에서는 축지법(1-step)으로 목적지로 이동

2. 250배 단계 단축의 진실: "속도도 250배 빨라지나요?"

이 논문의 가장 충격적인 수치는 연산 횟수(NFE)입니다.

 - DiT-XL/2 (기존): 250 steps
 - Drifting Model (Ours): 1 step

단순 계산으로 250배 효율적입니다. 하지만 여기서 냉정하게 봐야 할 '속도의 한계'가 있습니다. 250배 빨라지진 않습니다. 왜냐하면 '배보다 배꼽이 더 큰' 상황이 발생하기 때문입니다.


3. 성능: 더 작은 뇌로 더 잘 그린다

보통 1-step 모델은 품질이 떨어진다는 편견이 있습니다. 하지만 이 모델은 체급 차이를 실력으로 극복했습니다.

 - 덩치(파라미터): 경쟁 모델(DiT-XL, 675M)보다 훨씬 가벼운 463M 모델을 사용했습니다.
 - 화질(FID): 2.27(DiT) vs 1.54(Drifting). 숫자가 낮을수록 좋은데, 압도적인 차이로 이겼습니다.
 - 연산량(FLOPs): 기존 1-step 강자인 StyleGAN-XL 대비 연산량이 1/18 수준입니다. 가성비의 끝판왕입니다.

4. 작동 원리: 인력과 척력의 줄다리기

어떻게 한 번에 정답을 찾을까요? 연구진은 '표류장(Drifting Field)'이라는 개념을 도입했습니다.

 - 인력(Attraction): 실제 데이터 분포가 생성된 샘플을 끌어당깁니다.
 - 척력(Repulsion): 현재 생성된 샘플끼리는 서로 밀어냅니다.


💡 마치며: 내 노트북 속의 화가


하지만 시사점은 명확합니다. 우리는 그동안 GPU를 혹사시키며 250번, 1000번씩 계산하는 것을 당연하게 여겼습니다. Drifting Model은 "좋은 학습이란, 실전에서의 땀을 아끼게 해주는 것"이라는 사실을 증명했습니다.






likelove
114




