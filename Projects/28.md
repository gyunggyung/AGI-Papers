---
id: 28
category: Projects
title: vLLM과 SGLang의 핵심 기능을 llama.cpp에 심어서, llama.cpp 대비 최소 1.8배 빨라졌습니다. 이전 글에서 vLLM의 PagedAttention(공간 최적화)과 SGLang의 RadixAttention(시간 최적화)에 대해 설명드렸습니다. 그리고 생각했습니다. "이 좋은 걸 내 노트북 CPU 환경에서도 쓰고 싶은데?" 그래서 만들었습니다.
---
vLLM과 SGLang의 핵심 기능을 llama.cpp에 심어서, llama.cpp 대비 최소 1.8배 빨라졌습니다. 이전 글에서 vLLM의 PagedAttention(공간 최적화)과 SGLang의 RadixAttention(시간 최적화)에 대해 설명드렸습니다. 그리고 생각했습니다. "이 좋은 걸 내 노트북 CPU 환경에서도 쓰고 싶은데?" 그래서 만들었습니다. 

Agent.cpp: Tiny-MoA를 위한 전용 추론 엔진입니다.

🎯 왜 만들었나? (llama.cpp vs Agent.cpp)
기존 llama.cpp는 훌륭하지만, 에이전트를 사용할 때는 꽤 비효율적입니다. 특히 여러 에이전트가 대화하는 MoA(Mixture of Agents) 환경에서는, Brain이 다시 턴을 잡을 때마다 이전 대화 맥락을 새로 계산하느라 시간이 더 소요됩니다.

Agent.cpp는 이 병목을 해결하기 위해 최신 서빙 기술을 C++ 로컬 엔진에 이식했습니다.

1. RadixAttention (트리 기반 캐싱)
 - Brain이 반복해서 사용하는 시스템 프롬프트나, 이전 턴의 생각(Thought) 과정을 트리 구조로 저장해 즉시 불러옵니다.
 - 에이전트 간 대화의 족보를 트리(Tree)로 관리해, 필요한 기억만 쏙쏙 뽑아 씁니다.

2. MoA Native Orchestration
 - 파이썬 스크립트 없이 C++ 엔진 내부에서 Brain(지휘자)과 Specialist(전문가) 모델이 직접 소통하며 라우팅합니다.

🛠️ 솔직한 개발 후기 (Gemini & Vibe Coding)
사실 저는 C++ 고수와는 거리가 멉니다. 이번 프로젝트는 llama.cpp Clone이고 Google Antigravity에서 Gemini 3로 바이브 코딩한 결과물입니다. 솔직히 말하면 내부의 복잡한 포인터 연산이나 메모리 관리가 정확히 어떻게 돌아가는지 저는 모르겠습니다.

하지만 돌아갑니다. 그것도 아주 빠르고 효율적으로요. AI가 코딩해 준 AI 엔진인 셈입니다.

💡 마치며
수 천만 원짜리 GPU 서버가 없어도, 멋진 기술(Paged/Radix Attention)을 제 i5 16G RAM 윈도우 노트북에서 돌려보고 싶었습니다.

Agent.cpp는 그 욕망의 산물입니다. Tiny-MoA는 이제 더 빠른 뇌를 가지게 되었습니다.

🔗 GitHub: https://lnkd.in/gX3FDxfU
(MIT 라이센스로 공개합니다.)

🤖 Tiny MoA: https://lnkd.in/gmY8afKq
(이 프로젝트를 위해서 만들어봤습니다.)
58




