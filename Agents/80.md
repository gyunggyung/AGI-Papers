---
id: 80
category: Architecture
title: Adaptation of Agentic AI
---
Adaptation of Agentic AI

LlamaIndex KRLlamaIndex KR
하버드 등 12개 연구 기관이 합심해 Agentic AI에 대한 새로운 바이블을 내놓았습니다. 제목은 "Adaptation of Agentic AI". 쏟아지는 에이전트 연구들을 2x2 매트릭스로 완벽하게 정리했을 뿐 아니라, "거대 모델을 직접 튜닝하는 것보다, 도구를 튜닝하는 것이 훨씬 효율적이다"라는 중요한 통찰을 데이터로 증명합니다.

핵심적인 4가지 패러다임과 인사이트를 정리했습니다.

1. 적응의 4사분면: A1, A2, T1, T2

논문은 에이전트 시스템을 최적화하는 방법을 명쾌한 프레임워크로 나눕니다. 논문에서는 이를 '적응(Adaptation)'이라 정의합니다.

 - A2 (Agent Output Signaled Agent Adaptation): "결과만 좋으면 장땡". 도구 사용 과정보다 최종 답변의 품질(정답 여부)을 보고 에이전트를 고칩니다. 요즘 핫한 DeepSeek-R1이 여기에 속합니다.
 - T1 (Agent-Agnostic Tool Adaptation): "독립적인 전문가". 에이전트와 상관없이 도구 자체를 잘하게 만듭니다. (예: 일반적인 Dense Retriever)
 - T2 (Agent-Supervised Tool Adaptation): "비서실장 훈련시키기". 에이전트는 그대로 두고(Frozen), 이 에이전트 입맛에 딱 맞는 결과를 가져오도록 도구를 훈련시킵니다. (예: s3, AgentFlow)

2. 패러다임의 역전: 뇌를 고치지 말고, 손발을 맞춰라 (T2 > A2)

이 논문의 백미는 T2(도구 적응) 패러다임의 재발견입니다. 우리는 보통 에이전트(LLM) 자체를 학습시켜야(A2) 성능이 오를 거라 생각합니다. 하지만 논문은 이를 "비싼 비효율"이라 지적합니다. 여기서 말하는 '도구'는 단순한 API뿐만 아니라 1B~7B 수준의 가벼운 소형 모델(SLM)을 포함합니다.

 - A2 방식 (Search-R1): 에이전트(거대 모델)를 통째로 학습시키는 데 17만 개의 데이터가 필요했습니다.

 - 인사이트: 데이터 효율이 무려 70배 차이가 납니다. "똑똑한 리더(Frozen LLM) + 빠릿한 부하(Tuned Small Model)"의 조합이 정답이라는 것입니다. 논문에서는 이를 "공생적 역전(Symbiotic Inversion)"이라 부릅니다. 1B, 3B 모델들이 쓸모없는 것이 아니라, 이 시스템의 핵심 부품이 되는 것입니다.

3. 졸업(Graduation): 학생이 선생이 된다

또 하나 흥미로운 개념은 '졸업(Graduation)'입니다. A1/A2 방식으로 힘들게 훈련시킨 에이전트가, 나중에는 고정된(Frozen) 상태로 T1 도구처럼 쓰일 수 있다는 겁니다.


4. 훌륭한 팀플레이가 천재 한 명보다 낫다

A1, A2 방식은 '천재(LLM)' 한 명에게 모든 걸 가르치려다 보니, 이전에 배운 걸 까먹는 '파국적 망각(Catastrophic Forgetting)' 위험이 큽니다. 반면, T1, T2 방식은 기능을 모듈화하여 필요한 도구(기억 모듈, 계획 모듈 등)만 갈아끼우면 됩니다.

논문은 미래의 Agentic AI가 "거대한 단일 모델(Monolithic)"이 아니라, "고정된(Frozen) 강력한 추론 코어 + 지속적으로 진화하는 도구 생태계(Modular)"의 형태로 나아갈 것이라 전망합니다.

💡 마치며: 왜 이 논문을 읽어야 하는가


94




