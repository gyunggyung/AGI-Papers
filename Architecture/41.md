---
id: 41
category: Architecture
title: 불과 몇 년 전 LLM은 14B 정도는 돼야 쓸 수 있고, 32B는 넘어야 쓸만하다고 느꼈습니다. 그런데 2026년 1월, 이 통념을 정면으로 반박하는 모델이 등장했습니다. 90M, 600M의 크기로 뛰어난 지시 이행 능력과 수학 추론 능력을 갖춘 녀석들입니다.
---
불과 몇 년 전 LLM은 14B 정도는 돼야 쓸 수 있고, 32B는 넘어야 쓸만하다고 느꼈습니다. 그런데 2026년 1월, 이 통념을 정면으로 반박하는 모델이 등장했습니다. 90M, 600M의 크기로 뛰어난 지시 이행 능력과 수학 추론 능력을 갖춘 녀석들입니다. 

2023년 Falcon-180B를 오픈소스로 공개했던 Technology Innovation Institute(Falcon 팀)가 작정하고 깎아 만든 초소형 모델, Falcon-H1-Tiny 시리즈입니다.

이 모델은 단순히 "작게 만들었다"가 아닙니다. "뇌의 구조를 기형적으로 비틀었다"에 가깝습니다. 핵심 구현 디테일과 벤치마크 결과를 6가지로 정리했습니다.


1. 아키텍처: 뇌 용량(Width)을 포기하고, 깊이(Depth)를 늘리다

보통 90M~100M 급 모델은 레이어를 10~12개 정도 쌓습니다. 얕고 넓은 그릇이죠. 하지만 Falcon-H1-Tiny(90M)는 무려 50개의 레이어를 쌓았습니다. 이는 13B~30B 모델에서나 볼 수 있는 깊이입니다.

 - Extreme Deep & Narrow: 뇌의 용량(지식 저장공간)은 극도로 줄였지만, 사고하는 깊이(논리 회로)는 7B급 이상으로 확보했습니다.
 - Hybrid: 블록 내부에서 Mamba와 Attention이 병렬로 돌아갑니다. 요리사가 왼손으로 재료를 썰면서(Mamba: 문맥 파악), 동시에 오른손으로 간을 보는(Attention: 핵심 포착) 방식입니다.


2. 학습 전략: Anti-curriculum (반복의 미학)

데이터는 한 번만 봐야 한다(1 Epoch)는 상식을 뒤집었습니다.

 - 발견: 모델은 약 1,000억~5,000억 토큰(100~500GT) 이전에 본 데이터를 잊어버리는 '망각 창(Memorization Window)'이 있습니다.
 - 전략: 이 망각 창을 역이용해, 고품질 데이터(SFT급)를 100회 이상 반복 주입했습니다.
- 결과: 소량의 고품질 데이터를 뻥튀기해 총 8,000억(800B) 토큰을 학습시켰더니, 과적합 없이 성능이 수직 상승했습니다. 작은 모델일수록 "질 좋은 데이터를 완벽히 외울 때까지 반복하는 것"이 정답임을 증명했습니다.


3. 성능: 체급을 무시하는 하극상 (vs Llama, Gemma)

가장 놀라운 것은 14배 큰 모델을 이기는 지시 이행(Instruction)과, 3배 큰 모델을 압도하는 수학(Math) 능력입니다.

 - Instruction (IFEval): Falcon-90M(66.08) vs Llama-3.2-1B(52.37): 메타의 1B 모델보다 사용자의 까다로운 명령(JSON 포맷 준수, 요약 등)을 훨씬 잘 수행합니다.
 - Math (GSM8K): Falcon-90M(19.1) vs Gemma-270M(7.2): 구글의 최신 소형 모델보다 파라미터는 1/3 수준인데, 수학 문제 풀이 점수는 2.5배 더 높습니다.
 - Trend (LiveBench): Falcon-90M(15.7) vs SmolLM2-135M(8.2): 오염되지 않은 최신 난제 평가에서도 동급 경쟁자들을 2배 격차로 따돌렸습니다.



4. Function Calling: 기가 막힌 눈치, 아쉬운 꼼꼼함

이 모델을 에이전트 라우터로 쓸 수 있을까요? 90M Function Calling 모델의 성적표는 명확한 트레이드오프를 보여줍니다.

 - 눈치 (Relevance Detection): 94.44%. 사용자가 "날씨 알려줘"라고 했을 때 툴을 써야 한다고 판단하는 능력은 3배 큰 Gemma-270M(61%)을 압살합니다.
 - 꼼꼼함 (AST Accuracy): 36.06%. 하지만 막상 함수를 호출할 때 괄호를 빼먹거나 오타를 냅니다.
 - 교훈: 90M는 의도 파악에는 천재적이지만, 복잡한 스키마를 암기하기엔 용량이 부족합니다. 이를 해결하려면 350M 이상으로 체급을 키우거나, 외부 메모리가 필요합니다.


5. Optimization: 모델이 스스로 학습 속도를 정한다 (LRM)

Falcon 팀은 Muon 옵티마이저와 함께 LRM(Learnable Multipliers)이라는 기술을 도입했습니다. 사람이 학습률(Learning Rate)을 일일이 정해주는 게 아니라, 모델의 각 레이어가 "나는 중요하니까 더 크게 학습할래"라고 스스로 스케일을 조정하게 만든 것입니다. 이 기술 하나로 MMLU 점수가 20% 상승했습니다.


6. 라이선스: 상업적 이용 가능? Yes, but...

완전한 오픈소스(Apache 2.0)는 아닙니다. Falcon-LLM License를 따르는데, 이는 상업적 이용은 허용하되, 비윤리적(범죄, 감시 등) 사용은 금지한다는 조건부 라이선스입니다. 기업 서비스에 녹여내는 건 문제없지만, 배포 전 TII의 AUP(사용 정책)를 한 번쯤 확인해야 합니다.



Falcon-H1-Tiny는 모든 걸 아는 척척박사가 아닙니다. 지식(MMLU) 점수는 20점대로 처참합니다. 하지만 시키는 일을 빠릿하게 처리하는 똑똑한 비서로는 현존 최고의 가성비입니다.

여기서 지난번 소개한 DeepSeek의 Engram이 떠오릅니다.

 - Falcon: 50층 깊이의 논리력
 - Engram: 거대한 지식 저장소

이 둘을 결합한다면, 스마트폰에서도 돌아가는 100M~600M 크기로, 지식과 추론을 모두 갖춘 진정한 On-Device AI가 탄생하지 않을까요?


108




