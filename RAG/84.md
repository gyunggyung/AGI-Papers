---
id: 84
category: RAG
title: "From Code Foundation Models to Agents and Applications"
---
요즘 Claude Code나 Cursor 같은 코딩 도구를 많이들 쓰고 계시죠? 개발 생산성이 크게 향상된 것을 체감하면서도, 코딩 에이전트들을 어떻게 만든 건지 궁금해집니다. 그 궁금증을 해소해 줄 최선의 백서가 등장했습니다. 무려 303페이지, Hugging Face 논문 월간 트렌드 1위를 기록하며 화제가 된 논문입니다.

📄 제목: "From Code Foundation Models to Agents and Applications" 
👥 저자: 북경항공항천대학(Beihang Univ)을 중심으로 알리바바, 텐센트, 바이트댄스, 화웨이 등 중국 빅테크의 핵심 인력들이 총출동해 집필한 백서입니다.

이 논문은 단순 서베이가 아닙니다. 자율 코딩 에이전트 구현을 위한 엔지니어링 청사진입니다. 핵심 구현 디테일 6가지를 정리했습니다.

1. 패러다임의 전환: 망치에서 로봇으로 논문은 소프트웨어 개발의 역사를 도구의 진화 과정으로 명확히 정의합니다.

 - Human-Driven (1960~2010s): 인간이 직접 망치질을 하던 시기.

 - AI-Assisted (2021~): '전동 드릴(Copilot)'의 시대. 인간이 "여기 뚫어"라고 지시하면 AI가 실행만 합니다. 주도권은 여전히 인간에게 있습니다.

 - AI-Autonomous (2025~): "설계도만 주면 집을 짓는 로봇"의 시대 (Claude Code, Devin).

AI가 전체 프로젝트의 파일 구조를 파악하고, 의존성을 분석하며, 테스트-수정 루프를 스스로 돕니다. 이제 인간의 역할은 코딩에서 감독으로 이동합니다.


2. Data Curation: 코드 지능은 '순서'에서 나온다 "좋은 데이터를 넣으면 된다"는 말은 반만 맞습니다. 코드는 자연어와 구조가 다릅니다.

 - Topological Sort (위상 정렬): 일반 텍스트는 무작위로 학습해도 되지만, 코드는 아닙니다. 함수가 정의(Definition)되기 전에 호출(Call)되는 코드를 먼저 배우면 모델은 인과관계를 놓칩니다. 논문은 파일 간 의존성 그래프를 그려 학습 순서를 정렬하는 것이 모델의 '논리력'에 필수적임을 증명합니다.

 - Near-deduplication (MinHash): 변수명만 바꾼 똑같은 코드(Clone)가 GitHub엔 넘쳐납니다. MinHash와 Jaccard 유사도를 통해 의미적 중복을 제거하지 않으면 모델은 일반화 대신 '암기'를 하게 됩니다.



 - FIM (Fill-In-the-Middle): 모델은 코드를 앞에서 뒤로만 읽지 않습니다. 논문은 <PRE>(앞 코드) <SUF>(뒤 코드) <MID>(채울 내용) 형식으로 토큰을 재배열(PSM/SPM 모드)하여 학습시킵니다.

 - Repo-level Context: 단순히 현재 파일만 보지 않습니다. import 구문을 파싱하여 의존성이 있는 다른 파일의 코드 조각을 프롬프트 앞단에 붙여줍니다(Cross-file Context). 이것이 "다른 파일에 있는 함수"를 자동완성 할 수 있는 비결입니다.

 - Speculative Decoding: Tab 기능은 속도가 생명입니다. 논문은 작은 Draft Model(약 1B~3B)이 먼저 빠르게 토큰을 제안하고, 큰 모델이 검증하는 방식을 통해 지연 시간(Latency)을 획기적으로 줄이는 기법을 강조합니다.


4. Training Pipeline: SFT를 넘어 RLVR로 (DeepSeek의 비결) 이 논문이 꼽는 가장 강력한 Game Changer입니다.

 - Execution-Aware SFT: 단순히 "이 함수 짜줘"가 아닙니다. 코드와 함께 '그 코드를 검증할 테스트 케이스'를 쌍으로 학습시킵니다. 실행되지 않는 코드는 배우지 않습니다(RFT).

 - RLVR (Reinforcement Learning with Verifiable Rewards) ⭐: DeepSeek-R1이 증명한 방식입니다. 코딩은 Ground Truth가 명확합니다(컴파일 성공/실패). 이 명확한 이진 신호를 보상으로 삼아 강화학습을 돌리면, 모델은 수만 번의 시행착오 끝에 "스스로 코드를 고치고 검증하는 추론 능력"을 획득합니다.


5. Agent Memory: "어제 짠 코드를 기억하라" Claude Code가 프로젝트 전체를 이해하는 건 단순 RAG로는 불가능합니다. 논문은 계층적 메모리 구조를 제안합니다.

 - RepoMap & Tree-sitter: 코드를 텍스트 덩어리로 저장하지 않습니다. Tree-sitter를 사용해 AST(추상 구문 트리)를 파싱하고, 프로젝트의 폴더 구조와 핵심 클래스 관계를 요약한 RepoMap(트리 구조의 지도)을 메모리에 상주시킵니다.

 - Dynamic Context Pruning: 모든 코드를 다 넣으면 비용이 폭발합니다. 논문은 현재 작업과 관련 없는 파일은 과감히 가지치기(Pruning)하고, 필요한 코드 블록만 동적으로 로딩하는 전략을 상세히 다룹니다.


6. Evaluation: 점수판이 바뀌었다 (Pass@1 is Dead) 모델을 평가하는 기준도 완전히 달라졌습니다.

 - Static vs Dynamic: 단순히 텍스트 유사도(BLEU)를 보는 건 의미 없습니다. 논문은 모든 코드를 실제로 실행해서 평가해야 함을 강조합니다.

 - SWE-bench: 알고리즘 문제(HumanEval) 풀이는 이제 졸업했습니다. 실제 깃허브 이슈를 해결하고, PR을 날려 테스트를 통과하는지 평가합니다. 이것이 현재 모델들의 진짜 실력을 가르는 기준입니다.

 - Security Eval: 기능은 구현했지만 SQL 인젝션이나 버퍼 오버플로우 취약점을 만든다면 0점입니다. 논문은 CyberSecEval 등을 통해 보안성 검증이 필수 단계임을 명시합니다.


💡 이제 코딩은 AI가 합니다. 개발자의 역할은 "아키텍처 설계와 결과물 검증(Review)"이라는 고차원적 영역으로 이동하고 있습니다.




+2
100




