---
id: 51
category: Post_Training
title: 한국어 LLM을 학습시키려면 데이터가 필요합니다. 하지만 막상 찾아보면 영어 데이터는 넘쳐나는데, 한국어 데이터는 어디에 뭐가 있는지조차 파악하기 어렵습니다. 저도 새로운 모델을 학습 시키면서 이 문제를 뼈저리게 느꼈습니다. Pre-training 데이터는 어디서 구하지? SFT나 DPO는? GRPO를 돌리려면 어떤 데이터가 필요하지? 매번 허깅페이스를 뒤지고, 논문을 찾아보고, 커뮤니티를 돌아다녔습니다.
---
한국어 LLM을 학습시키려면 데이터가 필요합니다. 하지만 막상 찾아보면 영어 데이터는 넘쳐나는데, 한국어 데이터는 어디에 뭐가 있는지조차 파악하기 어렵습니다. 저도 새로운 모델을 학습 시키면서 이 문제를 뼈저리게 느꼈습니다. Pre-training 데이터는 어디서 구하지? SFT나 DPO는? GRPO를 돌리려면 어떤 데이터가 필요하지? 매번 허깅페이스를 뒤지고, 논문을 찾아보고, 커뮤니티를 돌아다녔습니다.

그래서 Claude, Gemini와 함께 정리했습니다. LLM-Ko-Datasets입니다.


📊 무엇이 들어있나요?

이 저장소는 한국어 LLM 학습에 필요한 데이터셋을 단계별로 정리한 큐레이션 리스트입니다.

- Pre-training: FineWeb, Nemotron-CC부터 한국어 위키피디아, 나무위키, korean_textbooks까지
- Mid-training: OLMo 3의 Dolmino 시리즈, 한국어 특화 Continued Pre-training 데이터
- SFT: KoCommercial-Dataset (1.44M), koVast (685K), Magpie-Pro-MT-300K-ko 등
- DPO/RLHF: ko_Ultrafeedback_binarized, orca-dpo-pairs-ko, K2-Feedback
- CoT/추론: NuminaMath-CoT-Ko, Yi-Sang (KOREAson) 5.79M prompts

2025년 최신 데이터셋(NVIDIA Nemotron, OLMo 3 Dolci 시리즈)까지 포함했습니다.



데이터가 부족하면 만들면 됩니다. 저장소에는 무료 번역 도구(Google Translate, NLLB, 제가 만든 lfm2-1.2b-koen-mt)를 활용해 영어 데이터를 한국어로 변환하는 전략도 정리해뒀습니다.

실제로 저는 1만 개의 데이터로 GRPO 학습을 돌렸고, 400 스텝만에 CHrF++ 34.61을 달성했습니다. 1.2B 모델이 Gemma-3-4B-it(32.83)를 넘었습니다. 데이터가 적어도, 전략이 있으면 됩니다.


🎯 왜 공개하나요?

한국어 LLM 생태계가 성장하려면, 데이터 접근성부터 해결되어야 한다고 생각합니다. 대기업처럼 수십억 원을 들여 데이터를 구축할 수 없는 개인 연구자, 스타트업, 학생들도 고품질 한국어 모델을 만들 수 있어야 합니다.

이 저장소가 도움이 되었으면 합니다.

Solar Open, K-EXAONE, A.X K1 등 한국 기업들의 기술 보고서에서 얻은 인사이트도 함께 정리했습니다. 합성 데이터(Synthetic Data)의 중요성, 커리큘럼 학습 전략, GRPO 기반 RL까지 실전에서 바로 적용할 수 있는 파이프라인도 제안합니다.


🔗 GitHub: https://lnkd.in/gJUxTQyN

새로운 데이터셋을 발견하시면 PR이나 Issue로 알려주세요. 함께 만들어가면 좋겠습니다.
188




