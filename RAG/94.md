---
id: 94
category: RAG
title: DeepSeek-V3 vs V3.2: 아키텍처의 진화
---
DeepSeek-V3 vs V3.2: 아키텍처의 진화

한계점: 기존 LLM은 컨텍스트 윈도우 크기에 의해 제한되며, 이를 초과하는 입력을 다루는 것이 어려움.

기존 접근법의 문제점: 추가 훈련이 필요하거나(RoPE 기반 확장), RAG와 같은 외부 모듈이 필요하거나, 실사용에서 성능 향상이 미미함.


주요 성과: Needle-In-a-Haystack(NIH) 테스트에서 1M 토큰 길이에서도 100% 정확도 달성. 기존 방법 대비 최대 288% 성능 향상.



---

2. 기존 연구 한계점 분석

컨텍스트 윈도우 확장 방식

GPT-4, Llama 3, Claude 3 등의 모델이 128K~1M 컨텍스트 지원하지만, 실제 성능은 기대에 못 미침.

컨텍스트 윈도우를 단순히 확장하는 것은 계산 비용 증가와 성능 저하 문제를 초래함.


대체 접근법

Positional Extrapolation & Interpolation: 위치 임베딩 조정으로 컨텍스트 윈도우 확장.

슬라이딩 윈도우(SWA): 입력을 작은 단위로 나누어 처리하는 방식.

KV 캐시 압축: 기존 KV 캐시를 효율적으로 관리하는 방법(H2O, SnapKV, PyramidKV 등).


한계점 정리

기존 접근법들은 높은 훈련 비용 또는 낮은 실질적 성능 향상의 한계를 가짐.




---

3. 제안 기법: InfiniRetri

핵심 개념: Attention 배분 패턴을 활용한 Retrieval-Augmented LLM.

작동 방식

입력을 Chunking + 슬라이딩 윈도우 방식으로 처리.

기존 KV 캐시를 이용하지 않고, 과거 토큰 ID를 저장하여 캐시로 활용.

Attention 분포를 분석하여 질문과 가장 관련성이 높은 토큰을 Retrieval.


Retrieval 과정

마지막 Attention 레이어의 다중 헤드 합산 후, 가장 높은 Attention 점수를 가진 토큰들을 선택.

문장 단위로 캐싱하여 문맥 정보를 유지하면서 장기적 문맥 유지 가능.




---

4. 실험 및 성능 비교

NIH(Needle-In-a-Haystack) Task 성능

기존 FullKV, StreamingLLM, SnapKV, PyramidKV 등 대비 압도적 성능.

Mistral-7B-Instruct의 컨텍스트 길이를 32K → 1M으로 확장하면서도 100% 정확도 유지.


LongBench 벤치마크 실험

QA Task에서 최대 369.6% 향상, 특히 HotpotQA에서 288% 성능 향상.



성능 최적화

기존 KV 캐시보다 낮은 연산량으로 성능 유지.





---

5. 결론 및 향후 연구 방향

기존 KV 캐시 기반 접근법을 대체할 수 있는 Retrieval 기반 장기 문맥 처리 모델.

컨텍스트 윈도우를 확장하는 것이 아니라, 작은 컨텍스트 내에서 모델의 내부 능력을 활용하는 새로운 패러다임 제시.

향후 연구 방향: 요약 태스크 개선, 더 정밀한 Retrieval 기법 적용 가능성 탐색.



---

총평

혁신성: 기존 RAG를 사용하지 않고도 attention 기반 retrieval을 수행하는 새로운 방법론.

실용성: 추가 훈련 없이 Transformer 기반 LLM에 적용 가능.


기대효과: 기존 LLM의 컨텍스트 윈도우 한계를 뛰어넘는 강력한 방법론으로, 장기 문맥 문제 해결에 기여할 가능성이 큼.

29




