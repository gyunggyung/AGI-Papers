---
id: 3
category: On_Device
title: LLM 지능의 민낯과 한계
---
LLM 지능의 민낯과 한계

LLM은 의사 면허 시험을 수석으로 통과했는데, 정작 환자를 만나면 엉뚱한 소리를 합니다. 최근 인공지능(LLM) 연구의 흐름을 보면, 우리가 지능이라고 믿었던 것들의 민낯이 드러나고 있습니다. 벤치마크 점수는 하늘을 찌르는데, 왜 현장에서는 구멍이 숭숭 뚫릴까요?

이번달 Nature Medicine과 TMLR에서 발표된 연구들은 이 기이한 현상인 '번역의 격차(Translation Gap)'를 정면으로 다루고 있습니다. 그리고 Fastino Labs는 이에 대한 공학적 해답을 제시했습니다.

LLM의 현재 한계와 해결책을 4가지 핵심 포인트로 정리했습니다.

1. 지식은 '만점', 진료는 '낙제' (Nature Medicine)

Oxford Internet Institute, University of Oxford 연구진이 1,298명을 대상으로 실험한 결과는 흥미롭습니다.

 - 시험(Encoding): 깨끗하게 정리된 의료 문제(SMS)를 풀게 하니 정확도가 94.9%에 달합니다. 거의 완벽하죠.
 - 실전(Generation): 하지만 일반 환자와 대화하는 챗봇 환경에서는 정확도가 34.5% 미만으로 수직 낙하합니다.

왜 이런 일이 벌어질까요? LLM의 고질병인 '아부(Sycophancy)'와 '유약함(Brittleness)' 때문입니다.

모델은 환자가 "저 체한 것 같아요"라고 잘못된 자가 진단을 내리면, 의학적 소견보다 환자의 말에 동조해 버립니다. 또한 질문의 어순을 조금만 바꿔도 답변이 완전히 달라지는 불안정함을 보였습니다.

2. 똑똑한 멍청이의 뇌 구조 (Caltech & Stanford University & Carleton College)

그렇다면 이건 단순히 학습 부족일까요? 스탠퍼드, 칼튼, 캘리포니아 공대 연구진은 이를 '구조적 결함'이라고 진단합니다.

 - 역전의 저주(Reversal Curse): 모델은 "A는 B다"를 배워도, "B는 A다"를 자동으로 떠올리지 못합니다. 인간에겐 당연한 논리가 모델에겐 없습니다.
 - 신체적 추론 실패: 그럴듯한 여행 계획을 짜주지만, 실제 물리 법칙이나 거리 감각이 없어 순간이동 수준의 동선을 제안합니다. (Hallucinating Physics)
 - 마음 이론 부재: 타인의 의도를 파악하는 사회적 지능이 부족해, 복잡한 협상이나 상담에서 맥락을 놓칩니다.

3. 해결책: 생성(Generation)하지 말고 추출(Extraction)하라

결국 문제는 "모든 걸 챗봇(Generation)으로 해결하려는 욕심"에 있습니다. Fastino Labs는 이 문제를 '적재적소의 아키텍처'로 해결합니다.

의사의 진료 기록처럼 정확성이 생명인 데이터에서 LLM에게 "요약해줘"라고 시키면 환각이 생깁니다. 대신 인코더 구조 GLiNER 같은 특화 모델을 써서 텍스트를 생성하는 게 아니라, 필요한 정보(증상, 약물, 진단명)만 '추출'해야 합니다.

 - 범용 LLM: 확률에 기반해 그럴듯한 말을 지어냄 (위험함)
 - GLiNER: 입력된 텍스트에서 팩트만 집어내 구조화함 (안전함, 빠름)

4. 속도와 보안, 두 마리 토끼

이런 특화 모델(Encoder) 접근법은 정확도만 높이는 게 아닙니다.

 - 속도: 무거운 생성 과정을 거치지 않아 CPU 환경에서 50~100ms 안에 처리가 끝납니다.
 - 보안: 외부 API로 데이터를 보낼 필요 없이, 로컬 환경에서 CPU만으로도 충분히 돌아갑니다.

💡 마치며: 백과사전은 의사가 아니다

지금의 LLM은 인류 역사상 가장 똑똑한 백과사전입니다. 하지만 백과사전을 통째로 외웠다고 해서 명의가 되는 건 아닙니다.

이번 연구들은 우리에게 중요한 교훈을 줍니다. "무조건 더 큰 모델, 더 많은 대화"가 정답이 아닐 수 있습니다. 오히려 AI의 역할을 창작과 사실 정리로 명확히 나누고, 도구에 맞는 아키텍처를 쓰는 것이 2026년의 AI 생존 전략이 될 것입니다.





63




