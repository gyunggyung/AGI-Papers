---
id: 48
category: Architecture
title: 2026년 새해, AMD의 Lisa Su는 CES 오프닝 키노트 무대 중앙으로 한 스타트업 CEO Ramin Hasani를 초대했습니다. OpenAI의 Greg Brockman도, Fei-Fei Li도 있었지만, 그녀가 모두를 위한 AI를 실현할 파트너로 지목한 곳은 바로 Liquid AI였습니다.
---
2026년 새해, AMD의 Lisa Su는 CES 오프닝 키노트 무대 중앙으로 한 스타트업 CEO Ramin Hasani를 초대했습니다. OpenAI의 Greg Brockman도, Fei-Fei Li도 있었지만, 그녀가 모두를 위한 AI를 실현할 파트너로 지목한 곳은 바로 Liquid AI였습니다.


1. 체급을 무시하는 지능: LFM 2.5-1.2B의 지시 이행(IFBench) 점수는 47.33입니다. 이는 1B 모델임에도 Claude 4 Sonnet(42.3), DeepSeek R1(38.0)을 넘어서며, Gemini 2.5 Pro(52.3)와도 비견될 수 있는 수치입니다.

2. 비현실적인 속도: 공개된 벤치마크에 따르면 갤럭시 S25 Ultra(CPU)에서 70 t/s, ROG Phone 9(NPU)에서는 무려 182 t/s를 기록했습니다. 사람이 읽는 속도보다 10배 이상 빠릅니다.

3. 근본이 다른 DNA: 이들은 오픈소스를 튜닝하는 흔한 스타트업이 아닙니다. MIT CSAIL 연구실에서 시작해 Nature Machine Intelligence에 논문을 싣고, 신경망의 근본 원리(Liquid Neural Networks)를 수학적으로 다시 설계한 팀입니다.



1. 아키텍처 심층 분석: 트랜스포머와 CNN의 하이브리드 미학

LFM 2 시리즈가 로컬 LLM의 패러다임을 바꾼 핵심은 "트랜스포머의 무거운 짐을 CNN(정확히는 LIV Conv)이 짊어지게 했다"는 점입니다. 이 설계는 인간의 직관이 아닌, Hardware-in-the-loop (HIL) 아키텍처 탐색이 찾아낸 엣지 디바이스의 최적 해입니다.

 - 10 대 6의 황금비: 1.2B 모델은 총 16개의 블록으로 구성됩니다. 그중 10개는 Conv(근육), 6개는 Attention(뇌)으로 배분했습니다. 기존 트랜스포머가 모든 층에서 무거운 Attention을 쓸 때, 이 모델은 단 6번만 사용해 메모리 폭발을 막았습니다.
 - 살아있는 가중치 (LIV Conv): 왜 유행하는 Mamba(SSM)를 안 썼을까요? SSM(Mamba)의 Scan 연산은 특정 하드웨어에서 최적화가 까다롭습니다. 반면 Liquid AI가 채택한 Short Convolution은 모든 CPU/NPU가 가장 잘하는 연산(SIMD/Cache Friendly)입니다. 덕분에 복잡한 전용 커널 없이도 O(N)의 속도와 캐시 효율을 동시에 잡았습니다.


2. 데이터의 퀀텀 점프: 28T 토큰의 압축 (Over-training)

LFM 2.5에서 가장 주목할 점은 학습 데이터의 양입니다. 보통 1B 모델은 2~3T 토큰 정도만 학습하는 게 관례였습니다.

 - LFM 2 (기존): 10T ~ 12T 토큰
 - LFM 2.5 (신규): 28T (28조) 토큰

무려 2.8배 늘어난 데이터를 1.2B라는 작은 그릇에 꾹꾹 눌러 담았습니다. 이는 모델의 지식 밀도를 극한으로 올리는 전략으로, 1B 모델이 멍청하다는 편견을 깨버린 물리적 기반입니다.


3. 학습의 정수: 단순 암기가 아닌 이해를 시키다 (RL & Distillation)

데이터만 많이 먹인다고 똑똑해지지 않습니다. LFM 2.5는 Decoupled Top-K Knowledge Distillation이라는 새로운 기법을 썼습니다.

 - Teacher 모델의 지식을 배울 때, 모든 단어를 다 따라 하는 게 아니라 중요한 상위 K개(Top-K)만 골라서 효율적으로 흡수했습니다. 확률의 크기(Mass)와 모양(Shape)을 분리해 학습하여 안정성을 높였습니다.
 - 여기에 SFT를 넘어 Multi-stage RL을 적용해, 지시 이행 능력(IFBench)을 15점대(Llama 3.2)에서 47점대로 3배 가까이 끌어올렸습니다.


4. 멀티모달 & 엣지: 진정한 온디바이스 AI

텍스트뿐만이 아닙니다. LFM 2.5-Audio는 자체 개발한 Compact Detokenizer를 탑재해 오디오 생성 속도를 기존 대비 8배나 높였습니다.

 - Whisper Large v3급 인식률과 GPT-4o급 대화 능력을 갖췄는데, 이걸 노트북 CPU로 돌릴 수 있다는 건 에이전트 개발자들에게 엄청난 기회입니다.


💡 마치며: LFM 실전 프로젝트


1. LFM 2 English-Korean Translation: 
Liquid AI의 Pau Labarta Bajo님이 공식 예제로 등록해주신 모델입니다. 1.2B의 크기로 Gemma-3-4B-it과 Qwen3-4B를 압도하는 번역 성능을 보여줍니다. 튜닝 과정에서 Maxime Labonne님께 많은 인사이트를 얻었습니다. (Special thanks to Pau and Maxime!)


3. Related Work 섹션 초안 생성기 LFM-Scholar:
연구 아이디어만 던지면 LFM 2-2.6B나 LFM 2.5-1.2B가 기존에 나온 관련 연구들을 자동으로 찾아서 Related Work 섹션 초안을 작성해줍니다.





46




