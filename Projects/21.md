---
id: 21
category: Projects
title: Insight Agents: An LLM-Based Multi-Agent System for Data Insights
---
모두가 "에이전트는 비싸고 느리다"고 말할 때, Amazon이 아주 고전적인 방법으로 이 문제를 해결했습니다. LLM 하나가 모든 걸 다 처리하게 두는 건, 마치 CEO에게 회사 로비에서 잡상인 차단부터 고객 안내까지 다 시키는 것과 같습니다. 

이 논문은 CEO(LLM) 앞에 아주 빠르고 똑똑한 '접수처'와 '가이드'를 세워 효율을 극대화했습니다.

제가 만들고 있는 Tiny-MoA에도 적용해야 되는, 아주 현실적이고 공학적인 접근입니다. 핵심 디테일 5가지로 정리했습니다.

📄 제목: Insight Agents: An LLM-Based Multi-Agent System for Data Insights 
👥 저자: Jincheng Bai et al. (Amazon) 

1. 1인 기업이 아니라, '분업화'된 조직 (Manager-Worker)

보통의 에이전트는 사용자의 질문을 곧바로 거대 모델(LLM)에 던집니다. 하지만 Insight Agents(IA)는 철저한 계층 구조를 택했습니다. 

 - 매니저(Manager): 입구에서 손님(질문)을 맞이합니다. 직접 일하지 않고 "이건 누구 담당이지?"만 판단합니다. 

 - 전문가(Workers): 데이터만 보여주는 직원(Data Presenter)과 분석을 해주는 직원(Insight Generator)이 따로 있습니다. 

 - 효과: 매니저가 교통 정리를 해주니, 각 전문가는 자기 일에만 집중할 수 있습니다. 

2. '모르는 것'을 구분하는 기술: 오토인코더(Auto-encoder)의 부활

재밌는 점은 이 매니저가 비싼 LLM이 아니라는 겁니다. 잡상인을 걸러내기 위해 아주 가벼운 '오토인코더'를 문지기(Gatekeeper)로 고용했습니다.

 - 원리: 시스템이 답변할 수 있는 '정상 질문'들의 패턴만 학습시킵니다. 

 - 작동: 학습한 적 없는 뚱딴지같은 질문(OOD)이 들어오면, 모델이 이를 복원하지 못하고 에러(재구성 오차)가 치솟습니다. 이 에러 값이 크면 "나 이거 몰라" 하고 0.009초 만에 쳐냅니다. 

 - 성능: 96.9%의 정밀도로 이상한 질문을 걸러냅니다. 무겁게 LLM을 돌려 "죄송합니다, 알 수 없습니다"라고 말하게 시키는 것보다 185배 빠릅니다. 

3. 33M 초경량 라우터: 가성비의 끝판왕 (BERT)

문지기를 통과한 질문을 '데이터 담당'으로 보낼지 '인사이트 담당'으로 보낼지 정하는 안내원(Router) 역할 역시 거대 모델을 쓰지 않았습니다.

 - 모델: bge-small-en-v1.5 (단 33M 파라미터) 

 - 비교: 요즘 흔한 7B 모델의 0.5%도 안 되는 크기입니다.

 - 결과: LLM에게 분류를 시키면 2초가 걸리는데, 이 녀석은 0.3초 만에 83%의 정확도로 분류를 끝냅니다. "분류(Classification)"는 생성 모델(LLM)보다 이해 모델(BERT)이 더 잘한다는 기본기를 증명했습니다. 

4. 비서의 센스: 쿼리 증강기 (Query Augmenter)

"지난주 판매량 어때?"라고 물으면 LLM은 당황합니다. '지난주'가 정확히 언제인지 모르기 때문이죠. 이때 '쿼리 증강기'가 개입합니다. 

 - 역할: 파이썬 코드 레벨에서 오늘 날짜를 확인하고, "2024년 4월 1일부터 4월 7일까지의 판매량"으로 질문을 구체화해서 LLM에게 넘깁니다. 

 - 비용: 모델이 아니라 단순 로직이라 비용이 '0원'입니다. 하지만 LLM의 답변 정확도는 비약적으로 올라갑니다.

5. 비용 0원에 도전하다: 데이터 증강의 마법

"이렇게 정교한 시스템을 만들려면 데이터가 많이 필요하지 않나?" 싶지만, 놀랍게도 거의 공짜에 가깝습니다.

 - OOD 학습: 답변 가능한 핵심 질문 178개만으로 학습했습니다. 

 - 라우터 학습: 178개의 원본 질문을 LLM으로 뻥튀기(Augmentation)해서, 각 카테고리당 300개 수준(총 600개 내외)으로 늘려 학습했습니다. 

 - 학습 비용: 모델이 워낙 작아서(33M), Colab 무료 버전으로 돌려도 몇 분이면 학습이 끝납니다.

 - 교훈: 돈으로 GPU를 사는 게 아니라, 구조(Architecture)를 잘 짜서 비용을 0에 가깝게 만들었습니다.

💡 마치며: 작고 빠른 것들의 협주곡

우리는 종종 "더 큰 모델, 더 많은 데이터"를 외치지만, 이 논문은 "적재적소"의 미학을 보여줍니다.

모든 판단을 비싼 LLM에게 미루지 마세요. 문 앞에는 빠릿빠릿한 오토인코더와 BERT를 세우고, LLM은 정말 복잡한 추론과 생성에만 집중하게 하세요. 

저 또한 Tiny-MoA 제작시 많은 휴리스틱 기법들을 적용했는데, 오토인코더와 BERT를 추가 해야겠습니다. LLM 라우터와 휴리스틱 기법은 생각보다 한계가 있습니다.

해당 논문에서는 claude-3-sonnet 기반의 에이전트를 만들었는데, 저처럼 Liquid AI LFM 2.5 1.2B 모델 기반의 에이전트를 만들 때는 작업 분해를 하는 과정에서 문제가 있습니다. 

모델이 작아(1.2B) 작업 분해 능력이 떨어지다 보니, 현재는 형태소 분석기 같은 고전적 룰(Rule)에 의존하고 있는데, 해당 논문처럼 더 나은 방식이 있는지 고민해봐야겠습니다.

결국 미래의 AI 시스템은 하나의 거대한 뇌가 아니라, 작지만 전문적인 여러 모델이 톱니바퀴처럼 맞물려 돌아가는 형태가 될 것입니다.

133




