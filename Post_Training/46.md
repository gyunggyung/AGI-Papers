---
id: 46
category: Post_Training
title: DeepSeek-R1의 등장으로 GRPO가 RL의 대세가 되었습니다. 하지만 세상에 완벽한 알고리즘은 없습니다. 특히 우리가 원하는 것이 "정답도 맞히고(내용), 포맷도 지켜라(형식)"처럼 여러 마리 토끼를 동시에 잡는 것이라면, GRPO는 치명적인 약점을 드러냅니다. 이 약점을 파고들어, Multi-reward RL 상황에서 GRPO를 압도하는 새로운 기법이 나왔습니다. NVIDIA와 HKUST가 발표한 GDPO입니다.
---
DeepSeek-R1의 등장으로 GRPO가 RL의 대세가 되었습니다. 하지만 세상에 완벽한 알고리즘은 없습니다. 특히 우리가 원하는 것이 "정답도 맞히고(내용), 포맷도 지켜라(형식)"처럼 여러 마리 토끼를 동시에 잡는 것이라면, GRPO는 치명적인 약점을 드러냅니다. 이 약점을 파고들어, Multi-reward RL 상황에서 GRPO를 압도하는 새로운 기법이 나왔습니다. NVIDIA와 HKUST가 발표한 GDPO입니다.

이 논문은 GRPO가 여러 보상을 뭉뚱그려 처리하다가 발생하는 정보 손실(Reward Collapse) 문제를 수학적으로 증명하고 해결책을 제시합니다. 핵심 포인트 5가지를 정리했습니다.

1. GRPO의 치명적 약점: "평균의 함정"

GRPO는 여러 가지 보상(정답 점수, 포맷 점수 등)을 일단 다 더한 뒤(Sum), 그룹 내에서 정규화합니다. 이걸 시험 점수에 비유해 볼게요.

 - 학생 A: 수학은 빵점(0점)인데, 글씨를 예쁘게 써서 가산점 받음(2점) = 합계 2점
 - 학생 B: 수학을 풀긴 했는데(1점), 글씨도 적당함(1점) = 합계 2점

GRPO의 수식에 넣으면 이 두 상황의 점수 차이가 희석되어, 모델 입장에선 "둘 다 똑같은 점수네?"라고 인식하는 보상 붕괴(Reward Collapse) 현상이 발생합니다. 수학 문제를 푼 노력이 무시당하는 겁니다.


2. GDPO의 해결책: "과목별 평가"

GDPO는 간단하지만 강력한 아이디어를 냅니다. "섞지 말고 과목별로 따로 등급을 매기자(Decoupled Normalization)"는 겁니다.

 - 수학 점수 따로 정규화
 - 글씨 점수 따로 정규화
 - 그 다음에 합치기

이렇게 하면 모델은 "아, A는 수학을 못했고, B는 수학을 했구나"라고 정확하게 학습 신호를 인지합니다. 논문은 이를 통해 학습 신호의 해상도(Resolution)를 높였다고 표현합니다.


3. 성능: 숫자가 증명하는 압도적 차이

이론이 좋아도 성능이 안 나오면 소용없겠죠? GRPO는 학습하다 보면 쉬운 보상(예: 길이 맞추기)만 챙기고, 어려운 보상(예: 수학 정답)은 포기하는 경향이 있습니다. GDPO는 다릅니다.

[수학 추론 (DeepSeek-R1-1.5B 기준)]

 - AIME 정확도: GRPO(23.1%) -> GDPO(29.4%)
무려 6.3%p나 상승했습니다. 모델 체급을 키우지 않고 알고리즘만 바꿔서 얻은 결과입니다.

[수학 추론 (Qwen3-4B-Instruct 기준)]

 - 길이 제약 위반율: GRPO(2.5%) -> GDPO(0.1%)
GRPO는 정답을 맞히려고 길이를 어기거나 그 반대였는데, GDPO는 위반율을 거의 없게 만들며 정답률도 높였습니다.

[코딩 (DeepSeek-R1-7B 기준)]

 - 버그 발생 비율: GRPO(3.9%) -> GDPO(2.5%)
코드를 짤 때도 컴파일 에러나 버그가 날 확률이 약 36% 감소했습니다.


4. Qwen 팀의 시선: "기본으로 돌아가라" (참고)

재미있는 건, 최근 알리바바 Qwen 팀도 비슷한 시기에 GRPO를 분석한 논문(Stabilizing RL)을 냈습니다.

이들은 GRPO가 사용하는 길이 정규화 같은 기교들이 수학적 근사(First-order Approximation)를 깨뜨린다고 지적합니다. 그래서 아예 기교를 뺀 가장 기초적인 방식(MiniRL)이 이론적으로는 더 깔끔하다고 하죠.

하지만 현실적인 성능과 멀티 태스킹이 중요한 우리에게는, 당장 GRPO의 약점을 보완해 주는 GDPO가 더 매력적인 선택지일 수 있습니다. 이론적 순수함보다는 당장 내 모델이 정답을 맞히는 게 중요하니까요.


5. 결론: 무엇을 써야 할까?

여러분의 프로젝트가 단 하나의 정답만 찾으면 된다면(Single Reward) GRPO도 훌륭합니다.

하지만 정답도 맞춰야 하고, 길이 제약도 지켜야 하고, 버그 없는 코드까지 짜야 하는 복잡한 요구사항(Multi-reward)을 수행해야 한다면? 숫자가 말해주듯 무조건 GDPO가 답입니다. (논문의 4.3절 Code Reasoning 실험: Pass + Length + Bug)

단, GDPO를 구현할 때는 논문에서 강조한 배치 정규화(Batch Normalization)를 꼭 넣어야 합니다. 이거 빼면 학습이 터집니다(Fail).


💡 이제 LLM Post-Training은 단순히 데이터를 먹이는 것을 넘어, "어떻게 보상을 설계하고 조합해서 모델을 납득시킬 것인가" 라는 게임 이론의 영역으로 넘어가고 있습니다.
likelovesupport
56




