---
id: 79
category: Pre_Training
title: LLM이 더 긴 문맥(Context)을 이해하도록 만드는 일에 모두가 매달리고 있지만, 결코 쉽지 않은 과제입니다. 푸단대 연구진은 시선을 돌려 가장 기초적인 부품부터 의심했습니다. "혹시 표준 기술인 RoPE가 스스로 정보를 유실하고 있었던 것은 아닐까?"
---
LLM이 더 긴 문맥(Context)을 이해하도록 만드는 일에 모두가 매달리고 있지만, 결코 쉽지 않은 과제입니다. 푸단대 연구진은 시선을 돌려 가장 기초적인 부품부터 의심했습니다. "혹시 표준 기술인 RoPE가 스스로 정보를 유실하고 있었던 것은 아닐까?"

이들은 `RoPE++(BEYOND REAL: IMAGINARY EXTENSION OF ROTARY POSITION EMBEDDINGS FOR LONG-CONTEXT LLMS)`라는 논문을 통해, 기존 방식이 편의성을 위해 버렸던 '정보의 절반(허수)'을 되살려 메모리 효율과 장기 기억력을 동시에 잡는 "영리한 복원 레시피"를 제시했습니다.

핵심적인 발견과 엔지니어링 디테일 그리고 한계점을 정리했습니다.

1. 표준의 함정: LLM의 GPS가 '반쪽'만 보고 있다

LLM은 텍스트를 읽을 때 단어의 '위치'를 알아야 합니다. "철수가 영희를 때렸다"와 "영희가 철수를 때렸다"는 단어 순서(위치)만 달라도 뜻이 정반대가 되죠.

이 역할을 하는 표준 기술이 `RoPE`입니다. 쉽게 말해 단어마다 고유한 각도(Angle)를 부여해 위치를 특정하는, LLM계의 GPS입니다.

하지만 기존 구현에는 치명적인 타협이 숨어있었습니다. RoPE는 수학적으로 복소수(Complex Number)를 기반으로 하는데, 복소수는 '실수(Real)'와 '허수(Imaginary)'가 한 세트입니다. 마치 위도와 경도가 있어야 정확한 좌표가 나오는 것과 같죠.

지금까지 개발자들은 "허수 계산은 복잡하다"는 이유로 실수부만 남기고 허수부를 버렸습니다. 우리는 지금까지 '위도'만 표시된 반쪽짜리 지도를 모델에게 쥐여주고 있었던 셈입니다.

2. 역할의 재정의: 돋보기(Real)와 망원경(Imaginary)

연구진은 버려진 허수부(Imaginary Part)를 분석한 결과, 놀라운 상보적 관계를 발견했습니다.

 - 실수 (기존 방식): 거리가 멀어질수록 값이 급격히 작아집니다(Local Decay). 가까운 문맥을 세밀하게 보는 '돋보기' 역할을 합니다.
 - 허수 (버려진 것): 거리가 멀어져도 정보가 선명하게 유지됩니다. 아주 먼 과거의 정보를 끌어오는 '망원경' 역할을 합니다.

즉, 모델이 긴 문맥(Long Context)을 잘 못 읽었던 건, 우리가 망원경을 압수했기 때문입니다. RoPE++는 이 망원경을 다시 찾아 끼우는 과정입니다.

3. 구현의 마법: "90도 회전" 트릭

가장 인상적인 엔지니어링 디테일입니다. "그럼 복잡한 복소수 연산 라이브러리를 새로 짜야 하나?"라는 우려를 아주 심플한 트릭으로 해결했습니다. 수학적으로 "데이터를 -90도 회전시키면, 그게 바로 허수 값이 된다"는 성질을 이용한 겁니다.

 - 상황: 복잡한 수식 변경 없이 허수 정보를 추출해야 합니다.
 - 해결: 입력 벡터를 90도 회전(Rotate)시켜서 기존 RoPE 함수에 넣습니다.
 - 결과: 코드 몇 줄 수정만으로, 연산 복잡도 증가 없이도 즉시 '허수 정보(망원경)'를 볼 수 있게 됩니다.

4. 효율성: 성능은 올리고, 메모리는 '반값'으로

보통 성능을 올리면 비용이 듭니다. 하지만 이 논문은 구조적 특성을 이용해 가성비까지 잡았습니다.

 - 성과: 이를 통해 KV Cache(메모리) 사용량을 기존 대비 50% 절감했습니다.
 - 의의: 성능(Long Context)은 좋아졌는데, 유지비(메모리)는 반값이 되었습니다. 엔지니어링 관점에서 가장 이상적인 트레이드오프입니다.

5. 한계와 경쟁: DeepSeek와 Llama 사이에서

이 논문은 훌륭하지만, 2025년 현재의 SOTA 기법들과 비교하면 명확한 포지셔닝 한계가 있습니다.

 - vs Llama 3.1 (Scaling): Llama 방식은 학습 없이 설정만 바꿔도 문맥이 늘어납니다(Plug & Play). 반면, RoPE++는 처음부터 다시 학습(Pre-training from Scratch)해야 하는 막대한 비용이 듭니다. 기존 모델을 튜닝해서 쓰고 싶은 팀에게는 그림의 떡입니다.

 - vs DeepSeek-V3.2 (MLA): DeepSeek의 MLA 구조는 아키텍처 레벨에서 압축을 수행해 메모리를 90% 이상 절감합니다. RoPE++의 50% 절감도 훌륭하지만, '극한의 가성비' 싸움에서는 MLA가 우위입니다.

💡 마치며: 100B 모델, 그리고 증명의 시간

아이디어는 훌륭하고 구현은 우아합니다. 하지만 가장 치명적인 약점은 "검증 스케일"입니다.

연구진은 논문에서 자원 제약으로 인해 최대 1.5B(15억) 파라미터 모델까지만 검증했다고 솔직히 밝혔습니다. 소형 모델에서의 성공이 100B, 400B 급 거대 모델에서도 그대로 재현될지는 미지수입니다. 거대 모델 특유의 창발성이나 학습 불안정성이 이 '회전 트릭'과 만났을 때 어떤 부작용을 일으킬지 모르기 때문입니다.

결국 RoPE++는 극한의 가성비를 쫓는 모델보다는, 수학적 완전성과 문맥 이해의 깊이를 추구하는 "차세대 고성능 파운데이션 모델"을 바닥부터 설계하려는 연구자에게 좋은 레시피가 될 것입니다. 하지만 당장 서비스를 개선해야 하는 엔지니어에게는 적용하기 어려운 '미래의 청사진'에 가깝습니다.
53

love
마음에 쏙듬



