---
id: 44
category: Architecture
title: 딥시크가 또 하나의 중요한 화두를 던졌습니다. 이번에는 기억(Memory)입니다. 우리는 그동안 LLM의 성능을 높이기 위해 모델을 키우거나(Dense), 전문가를 나누는(MoE) 방식에 집중했습니다. 하지만 DeepSeek AI팀은 근본적인 비효율을 지적합니다. "왜 단순한 사실을 말하기 위해 복잡한 연산을 낭비하는가?" DeepSeek가 제안한 새로운 아키텍처, Engram에 대한 논문입니다. 이 논문은 LLM의 새로운 희소성 축(New Axis of Sparsity)을 제시합니다.
---
딥시크가 또 하나의 중요한 화두를 던졌습니다. 이번에는 기억(Memory)입니다. 우리는 그동안 LLM의 성능을 높이기 위해 모델을 키우거나(Dense), 전문가를 나누는(MoE) 방식에 집중했습니다. 하지만 DeepSeek AI팀은 근본적인 비효율을 지적합니다. "왜 단순한 사실을 말하기 위해 복잡한 연산을 낭비하는가?" DeepSeek가 제안한 새로운 아키텍처, Engram에 대한 논문입니다. 이 논문은 LLM의 새로운 희소성 축(New Axis of Sparsity)을 제시합니다.

핵심 구현 디테일과 인사이트를 5가지로 정리했습니다.

1. 패러다임의 전환: "계산하지 말고, 컨닝하세요"

기존 모델들은 "프랑스의 수도는?"이라는 질문에 답하기 위해 수많은 레이어의 Attention과 FFN 연산을 거칩니다. 마치 구구단을 외워서 답하는 게 아니라, 매번 숫자를 더해서 계산하는 것과 같은 낭비입니다.

DeepSeek은 이를 해결하기 위해 조건부 연산(MoE)에 이어 조건부 기억(Engram)이라는 개념을 도입했습니다.

 - MoE (Conditional Computation): 복잡한 추론을 위해 특정 전문가(Expert) 신경망을 호출합니다. (뇌를 사용)
 - Engram (Conditional Memory): 자주 등장하는 패턴이나 지식은 거대 임베딩 테이블에서 바로 찾아옵니다. (컨닝 페이퍼를 사용)


2. 성능: 뇌 용량을 아꼈더니, 머리가 더 좋아졌다

연구진은 동일한 파라미터(27B)와 연산량(Iso-FLOPs)을 가진 MoE 모델과 Engram 모델을 비교했습니다. 결과는 압도적입니다.

 - 지식 & 추론 급상승: 단순 암기력(MMLU +3.4)만 좋아진 게 아닙니다. 복잡한 추론(BBH +5.0), 코딩(HumanEval +3.0), 수학(MATH +2.4) 능력까지 대폭 향상되었습니다.


3. 무한 메모리(Infinite Memory): 속도 저하 없는 확장

이 논문의 백미는 시스템 효율성입니다. Engram 테이블은 GPU VRAM이 아니라 CPU 메모리(RAM)에 둬도 됩니다.

 - Prefetching: MoE는 계산해 봐야 다음 전문가를 알 수 있지만, Engram은 입력 텍스트만 보면 무엇을 찾아야 할지 미리 알 수 있습니다(Deterministic).
 - Zero Overhead: GPU가 연산하는 동안 CPU가 미리 기억을 찾아 대령합니다. 실험 결과, 1,000억(100B) 파라미터 크기의 메모리 테이블을 붙여도 추론 속도 저하는 3% 미만이었습니다.


4. 소형 모델(SLM)의 구원투수


 - 가성비의 혁명: 100M~500M 급의 작은 모델은 지식을 담기에 용량이 부족합니다. 여기에 1B~10B 규모의 Engram 메모리를 붙이면, "속도는 소형 모델처럼 빠르지만, 지식은 대형 모델처럼 풍부한" 하이브리드 모델을 만들 수 있습니다.
 - Log-linear 법칙: 논문은 연산량을 늘리지 않고 메모리 슬롯만 늘려도 성능이 계속 좋아진다는 것을 증명했습니다.


5. 적용 가이드: From Scratch vs Continual

논문은 바닥부터 학습(From Scratch)하는 것을 기준으로 작성되었습니다. 하지만 이미 학습된 모델을 가진 우리에게도 길은 있습니다.

 - Zero Initialization: Engram 모듈을 기존 모델 사이에 끼워 넣되, 초기 출력값을 0으로 설정하면 수학적으로 기존 모델과 동일한 상태에서 시작할 수 있습니다.
 - 전략: 이미 똑똑한 모델(Pre-trained)에게 위키피디아 같은 지식 데이터만 집중적으로 보여주며 Engram 테이블을 채우는 추가 학습(Continual Pre-training) 전략이 매우 유효할 것으로 보입니다.


💡 마치며: A New Axis

DeepSeek은 작년 말 DeepSeek V3.2로 효율적인 LLM의 새로운 기준을 세웠고, 얼마 전에는 mHC로 ResNet의 10년 묵은 한계를 해결했습니다. 그리고 이번에는 기억(Memory)입니다.

우리는 그동안 연산(Neural)의 효율화에 집중했습니다. 하지만 Engram은 우리에게 새로운 방향을 제시합니다. "더 큰 모델을 만들지 말고, 더 큰 기억을 달아줘라."






216




