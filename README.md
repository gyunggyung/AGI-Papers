<h1 align="center"> üåü  NLP-Papers üåü </h1>

<p align="center">
  </a> 
    </a>
  <em>
    NLP
    ¬∑ NLU
    ¬∑ NLG
  </em>
  <br />
  <em>
    Summarization
    ¬∑ Sentiment analysis
  </em>
  <br />
  <em>
    NER
    ¬∑ POS
    ¬∑ MNT
    ¬∑ QA
  </em>
  <br />
  <em>
    Text categorization 
    ¬∑ Semantic parsing
  </em>
  <br />
  
  <em>
    <a href="https://github.com/gyunggyung/PyTorch">
      GOTO PyTorch!
    </a>
  </em>
</p>

<p align="center">
  <a href="https://opensource.org/licenses/MIT">
    <img alt="licenses" src="https://img.shields.io/github/license/gyunggyung/NLP-Papers?style=flat-square"></a>
  <a href="https://github.com/gyunggyung/NLP-Papers/stargazers">
    <img alt="GitHub stars" src="https://img.shields.io/github/stars/gyunggyung/NLP-Papers?style=flat-square&color=yellow"></a>
  <a href="https://github.com/gyunggyung/NLP-Papers/blob/master/watchers">
    <img alt="GitHub watching" src="https://img.shields.io/github/watchers/gyunggyung/NLP-Papers?style=flat-square&color=ff69b4"></a>
  <a href="https://github.com/gyunggyung/NLP-Papers/graphs/contributors">
    <img alt="contributors" src="https://img.shields.io/badge/contributors-welcome-yellowgreen?style=flat-square"></a>
</p>

<div align="center">
    <sub> Let's find out the latest and various NLP-related papers. üôá‚Äç‚ôÇÔ∏èüôá‚Äç‚ôÄÔ∏è by <a href="https://github.com/gyunggyung/NLP-Papers/stargazers">Stargazers</a>  </sub>
</div>


## Papers

- [2013/01] **[Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)**    

- [2014/12] **[Dependency-Based Word Embeddings](https://www.aclweb.org/anthology/P14-2050.pdf)**

- [2015/07] **[Neural Machine Translation of Rare Words with Subword Units](https://www.aclweb.org/anthology/P16-1162.pdf)**

- [2014/07] **[GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf)** : *GloVe*

- [2016/06] **[Siamese CBOW: Optimizing Word Embeddings for Sentence Representations](https://arxiv.org/pdf/1606.04640.pdf)** : *Siamese CBOW*

- [2016/07] **[Enriching Word Vectors with Subword Information](https://arxiv.org/pdf/1607.04606.pdf)** : *fastText*

- [2014/09] **[Sequence to Sequence Learningwith Neural Networks](https://arxiv.org/pdf/1409.3215.pdf)** : *seq2seq*

- [2017/07] **[Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)** : *Transformer*

- [2017/08] **[Learned in Translation: Contextualized Word Vectors](http://papers.nips.cc/paper/7209-learned-in-translation-contextualized-word-vectors.pdf)** : *CoVe*

- [2018/01] **[Universal Language Model Fine-tuning for Text Classification](https://arxiv.org/pdf/1801.06146.pdf)** : *ULMFIT*

- [2018/02] **[Deep contextualized word representations](https://arxiv.org/pdf/1802.05365.pdf)** : *ELMo* 

- [2018/06] **[Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)** : *GPT-1* 

- [2018/10] **[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)** : *BERT*     

- [2019/02] **[Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)** : *GPT-2* 

- [2019/04] **[Language Models with Transformers](https://arxiv.org/abs/1904.09408)** 

- [2019/08] **[Neural Text Generation with Unlikelihood Training](https://arxiv.org/pdf/1908.04319.pdf)** 

- [2019/01] **[Cross-lingual Language Model Pretraining](https://arxiv.org/pdf/1901.07291.pdf)** *XLM* 

- [2019/01] **[Multi-Task Deep Neural Networks for Natural Language Understanding](https://arxiv.org/pdf/1901.11504.pdf)** : *MT-DNN*    

- [2019/01] **[Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860)** : *Transformer-XL*    

- [2019/06] **[XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237)** : *XLNet*

- [2019/04] **[The Curious Case of Neural Text Degeneration](https://arxiv.org/pdf/1904.09751.pdf)**

- [2019/09] **[Fine-Tuning Language Models from Human Preferences](https://arxiv.org/abs/1909.08593)** 

- [2019/01] **[BioBERT: a pre-trained biomedical language representation model for biomedical text mining](https://arxiv.org/ftp/arxiv/papers/1901/1901.08746.pdf)** : *BioBERT* 

- [2019/03] **[SciBERT: A Pretrained Language Model for Scientific Text](https://arxiv.org/abs/1903.10676.pdf)** : *SciBERT*

- [2019/04] **[ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission](https://arxiv.org/abs/1904.05342.pdf)** : *ClinicalBERT* 

- [2019/06] **[HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization](https://arxiv.org/pdf/1905.06566.pdf)** : *HIBERT* 

- [2019/07] **[SpanBERT: Improving Pre-training by Representing and Predicting Spans](https://arxiv.org/abs/1907.10529)** : *SpanBERT*

- [2019/04] **[Publicly Available Clinical BERT Embeddings](https://arxiv.org/abs/1904.03323.pdf)** 

- [2019/08] **[Pre-Training with Whole Word Masking for Chinese BERT](https://arxiv.org/pdf/1906.08101.pdf)** 

- [2019/07] **[Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment](https://arxiv.org/pdf/1907.11932.pdf)**


- [2019/07] **[R-Transformer: Recurrent Neural Network Enhanced Transformer](https://arxiv.org/abs/1907.05572)** : *R-Transformer*


- [2019/09] **[FREELB: ENHANCED ADVERSARIAL TRAINING FOR LANGUAGE UNDERSTANDING](https://arxiv.org/pdf/1909.11764.pdf)** : *FREELB*

- [2019/09] **[Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks](https://arxiv.org/pdf/1909.11515.pdf)**


- [2019/10] **[Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf)** : *T5*



- [2018/07] **[Subword-level Word Vector Representations for Korean](https://www.aclweb.org/anthology/P18-1226.pdf)**



- [2019/08] **[Zero-shot Word Sense Disambiguation using Sense Definition Embeddings](https://malllabiisc.github.io/publications/papers/EWISE_ACL19.pdf)**

- [2019/06] **[Bridging the Gap between Training and Inference for Neural Machine Translation](https://arxiv.org/pdf/1906.02448.pdf)**

- [2019/06] **[Emotion-Cause Pair Extraction: A New Task to Emotion Analysis in Texts](https://arxiv.org/pdf/1906.01267.pdf)**

- [2019/07] **[A Simple Theoretical Model of Importance for Summarization](https://www.aclweb.org/anthology/P19-1101.pdf)**

- [2019/05] **[Transferable Multi-Domain State Generator for Task-Oriented Dialogue Systems](https://arxiv.org/pdf/1905.08743.pdf)**

- [2019/07] **[We need to talk about standard splits](http://wellformedness.com/papers/gorman-bedrick-2019.pdf)**

- [2019/07] **[ERNIE 2.0: A Continual Pre-training Framework for Language Understanding](https://arxiv.org/abs/1907.12412v1.pdf)** : *ERNIE 2.0*

- [2019/05] **[SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems](https://arxiv.org/pdf/1905.00537.pdf)** : *SuperGLUE*

- [2020/01] **[Towards a Human-like Open-Domain Chatbot](https://arxiv.org/pdf/2001.09977.pdf)** + *[Google AI Blog](https://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.html)*

- [2020/03] **[ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://arxiv.org/abs/2003.10555.pdf)** : *ELECTRA*

- [2019/04] **[Mask-Predict: Parallel Decoding of Conditional Masked Language Models](https://arxiv.org/abs/1904.09324.pdf)** : *Mask-Predict*

- [2020/01] **[Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451.pdf)** : *Reformer*

- [2020/04] **[Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150.pdf)** : *Longformer*

- [2019/11] **[DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation](https://arxiv.org/abs/1911.00536.pdf)** : *DialoGPT*

- [2020/01] **[Towards a Human-like Open-Domain Chatbot](https://arxiv.org/abs/2001.09977.pdf)**

- [2020/04] **[You Impress Me: Dialogue Generation via Mutual Persona Perception](https://arxiv.org/abs/2004.05388.pdf)**

- [2020/04] **[Recipes for building an open-domain chatbot](https://arxiv.org/abs/2004.13637.pdf)**

- [2020/04] **[ToD-BERT: Pre-trained Natural Language Understanding for Task-Oriented Dialogues](https://arxiv.org/abs/2004.06871.pdf)**
: *ToD-BERT*

- [2020/04] **[SOLOIST: Few-shot Task-Oriented Dialog with A Single Pre-trained Auto-regressive Model](https://arxiv.org/abs/2005.05298.pdf)** : *SOLOIST*

- [2020/05] **[A Simple Language Model for Task-Oriented Dialogue](https://arxiv.org/abs/2005.00796.pdf)**

- [2019/07] **[ReCoSa: Detecting the Relevant Contexts with Self-Attention for Multi-turn Dialogue Generation](https://arxiv.org/abs/1907.05339.pdf)** : *ReCoSa*

- [2020/04] **[FastBERT: a Self-distilling BERT with Adaptive Inference Time](https://arxiv.org/abs/2004.02178)** : *FastBERT*

- [2020/01] **[PoWER-BERT: Accelerating BERT inference for Classification Tasks](https://arxiv.org/abs/2001.08950)** : *PoWER-BERT*

- [2019/10] **[DistillBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108)** : *DistillBERT*

- [2019/10] **[TinyBERT: Distilling BERT for Natural Language Understanding](https://arxiv.org/abs/1909.10351)** : *TinyBERT*

- [2019/11] **[Not Enough Data? Deep Learning to the Rescue!](https://arxiv.org/abs/1911.03118)**

- [2018/12] **[Conditional BERT Contextual Augmentation](https://arxiv.org/abs/1812.06705)**

- [2020/03] **[Data Augmentation using Pre-trained Transformer Models](https://arxiv.org/abs/2003.02245)**

- [2020/04] **[FLAT: Chinese NER Using Flat-Lattice Transformer](https://arxiv.org/abs/2004.11795)** : *FLAT*

- [2019/12] **[Big Transfer (BiT): General Visual Representation Learning](https://arxiv.org/abs/1912.11370)** : *BiT*

- [2019/04] **[ERNIE: Enhanced Representation through Knowledge Integration](https://arxiv.org/abs/1904.09223)** : *ERNIE*

- [2019/07] **[ERNIE 2.0: A Continual Pre-training Framework for Language Understanding](https://arxiv.org/abs/1907.12412)** : *ERNIE 2.0*

- [2020/06] **[ERNIE-ViL: Knowledge Enhanced Vision-Language Representations Through Scene Graph](https://arxiv.org/abs/2006.16934)** : *ERNIE-ViL*

- [2020/12] **[ERNIE-Doc: A Retrospective Long-Document Modeling Transformer](https://arxiv.org/abs/2012.15688)** : *ERNIE-Doc*

- [2021/07] **[ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation](https://arxiv.org/abs/2107.02137)** : *ERNIE 3.0*

- [2022/10] **[Beyond English-Centric Bitexts for Better Multilingual Language Representation Learning](https://arxiv.org/abs/2210.14867)**

- [2017/03] **[Distilling Task-Specific Knowledge from BERT into Simple Neural Networks](https://arxiv.org/abs/1903.12136)**

- [2020/10] **[DiPair: Fast and Accurate Distillation for Trillion-Scale Text Matching and Pair Modeling](https://arxiv.org/abs/2010.03099)** : *DiPair*

- [2021/08] **[Distilling Transformers for Neural Cross-Domain Search](https://arxiv.org/abs/2108.03322)**

- [2020/06] **[DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654)** : *DeBERTa*

- [2020/11] **[VEGA: Towards an End-to-End Configurable AutoML Pipeline](https://arxiv.org/abs/2011.01507)** : *VEGA*

- [2020/12] **[FILTER: An Enhanced Fusion Method for Cross-lingual Language Understanding](https://arxiv.org/abs/2009.05166)** : *FILTER*

- [2019/12] **[StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding](https://arxiv.org/abs/1908.04577)** : *StructBERT*

- [2019/04] **[Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding](https://arxiv.org/abs/1904.09482)** : *MT-DNN*

- [2021/05] **[Comparing Kullback-Leibler Divergence and Mean Squared Error Loss in Knowledge Distillation](https://arxiv.org/abs/2105.08919)**

- [] **[]()** : **

## Basic knowledge

| mathematics                                                                          | machine learning                                                                                                                                                                            |
|:------------------------------------------------------------------------------------:|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|
| [![](image/ml.PNG)](https://mml-book.github.io/) | [![](image/pr.PNG)](http://users.isr.ist.utl.pt/~wurmd/Livros/school/) |
| **[mathematics for machine learning](https://mml-book.github.io/)** | **[Pattern Recognition and Machine Learning](http://users.isr.ist.utl.pt/~wurmd/Livros/school/)** |
