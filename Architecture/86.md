---
id: 86
category: Architecture
title: 최근 화제가 된 Mistral Large 3와 Kimi K2를 분석해보면, 그 기반에는 DeepSeek V3 아키텍처가 있었습니다. 이제 겨우 업계가 V3 구조를 표준처럼 받아들이고 따라잡나 싶었는데, DeepSeek AI 팀이 V3.2를 통해 기술적 목표점을 다시 한번 높여버렸습니다.
---
최근 화제가 된 Mistral Large 3와 Kimi K2를 분석해보면, 그 기반에는 DeepSeek V3 아키텍처가 있었습니다. 이제 겨우 업계가 V3 구조를 표준처럼 받아들이고 따라잡나 싶었는데, DeepSeek AI 팀이 V3.2를 통해 기술적 목표점을 다시 한번 높여버렸습니다.

이번 논문의 핵심은 효율성(Efficiency)입니다. 단순히 성능 스코어를 높인 것이 아니라, 긴 문맥에서의 비용 구조를 근본적으로 혁신했습니다.

성능이 GPT-5급이라는 마케팅적 수사보다, 이들이 어텐션(Attention)을 재설계한 방식이 더 흥미롭습니다.

1. DSA: 도서관의 책을 전부 정독하지 마라 (O(L²) → O(L*k))

기존의 어텐션(Dense Attention)은 질문에 답하기 위해 지금까지의 모든 문맥(Context)을 처음부터 끝까지 정독합니다. 길이가 길어지면 연산량은 길이의 제곱(L의 2승)으로 폭발합니다.

DeepSeek는 "필요한 것만 골라 읽자(Sparse)"는 전략을 택했습니다. 하지만 무엇이 중요한지 어떻게 판단할까요? 여기서 두 가지 핵심 장치가 등장합니다.

(1) 라이트닝 인덱서 (Lightning Indexer): 초고속 정찰병

쿼리가 들어오면, 무거운 메인 모델이 연산을 시작하기 전에 가벼운 정찰병을 먼저 보냅니다.

 * 이 인덱서는 FP8로 동작하며 연산 비용이 매우 낮은 ReLU 함수를 사용합니다.

 * 전체 문맥을 빠르게 훑으며 현재 질문과 관련도가 높은 구간에 점수를 매깁니다.

(2) 정밀 선택 (Fine-grained Selection)

정찰병이 선별한 상위 k개의 블록만 메인 모델이 가져와서 정밀하게 연산합니다.

 * 결과: 12만 토큰(L=128K)을 처리할 때도, 실제로는 핵심적인 2천 개(k=2K) 내외의 토큰만 집중해서 봅니다.

 * 복잡도 혁신: 이에 따라 메인 어텐션의 연산 복잡도가 기존 O(L²)에서 O(L*k)로 획기적으로 줄어듭니다. (k는 L보다 훨씬 작음)

 * 비용 혁신: 논문 데이터(Figure 3)에 따르면, 128K 문맥에서 프리필링 비용은 기존 대비 약 3분의 1로, 디코딩 비용은 약 10분의 1 수준으로 감소했습니다. 연산량 그래프가 수직 상승하는 대신 평탄하게 유지됩니다.


2. 학습의 정교함: 구조적 변화에 대한 연착륙
어텐션을 갑자기 희소(Sparse)하게 변경하면 모델은 문맥을 놓치기 쉽습니다. 논문은 이 문제를 '2단계 학습'으로 해결했습니다.

 * 1단계 (Dense Warm-up): 초기에는 모델 파라미터를 고정하고 인덱서만 학습시킵니다. 기존 Dense 모델이 주목하던 위치를 모사하게 하여 인덱서의 선별 능력을 먼저 키웁니다.

 * 2단계 (Sparse Training): 인덱서의 정확도가 확보되면, 그때부터 실제 희소 모드를 활성화하고 모델 전체를 학습시킵니다.

이렇게 디테일한 엔지니어링 덕분에 긴 문맥에서의 성능 저하 없는 효율을 달성했습니다.


3. 강화학습(RL)은 튜닝이 아니라 필수 공정이다

자원 배분 전략 또한 시사하는 바가 큽니다. DeepSeek는 사후 학습(RL) 단계에 사전 학습(Pre-training) 비용의 10% 이상을 투입했습니다.

수학, 코딩 등 각 분야 전문가 모델을 먼저 만들고(SFT), 이를 통합한 뒤 대규모 RL을 통해 지능의 밀도를 극한까지 높였습니다. 

그 결과물인 Speciale 버전은 수학(IMO)과 코딩(IOI) 올림피아드에서 금메달 수준의 성과를 냈습니다. 데이터의 양보다 '학습의 강도'가 추론 능력에 미치는 영향을 증명한 셈입니다.


4. 도구를 든 철학자 (Thinking in Tool-Use)

에이전트 기능에서의 문맥 관리(Context Management)도 주목할 만합니다. 

기존 모델들은 도구를 호출하거나 API 결과를 기다리는 동안 추론의 맥락을 잃는 경우가 많았습니다.

DeepSeek-V3.2는 도구를 사용하는 전 과정에서 '생각의 끈(Reasoning Trace)'을 문맥에 계속 유지합니다. 


마치며: 효율성이 곧 경쟁력이다

모든 정보를 다 기억하고 연산할 필요는 없습니다. 중요한 건 '필요한 정보를 얼마나 적은 비용으로 찾아내느냐(DSA)', 그리고 '얼마나 깊이 있게 추론하느냐(RL)'입니다. 하드웨어의 제약을 소프트웨어 아키텍처의 혁신으로 돌파하는 모습, 이것이 진정한 엔지니어링의 정수라 생각합니다.
60




