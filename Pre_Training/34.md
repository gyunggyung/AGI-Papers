---
id: 34
category: Pre_Training
title: Continuous Autoregressive Language Models (CALM)
---
우리는 왜 2026년에도 LLM이 한 글자씩 타이핑하는 걸 지켜봐야 할까요? 강력한 GPU를 쓰면서 고작 단어 조각(Token) 하나씩, 찔끔찔끔 뱉어내는 이 비효율. "이걸 한 번에 4개씩, 8개씩 뱉게 만들면 안 되나?"라는 단순하지만 대담한 상상이 현실이 되었습니다.

오늘 소개할 논문은 기존의 '다음 토큰 예측(Next-Token Prediction)'이라는 딥러닝의 십계명을 깨부수고, '다음 벡터 예측(Next-Vector Prediction)'이라는 새로운 패러다임을 제시한 CALM입니다.

📄 제목: Continuous Autoregressive Language Models (CALM)
👥 저자: 텐센트(Tencent) AI Lab & 칭화대(Tsinghua University)

단순히 속도만 빠른 게 아닙니다. 모델을 키우는 새로운 축(Axis)을 발견했습니다. 핵심 디테일 4가지로 정리했습니다.

1. 한 글자 타이핑(Discrete)에서 도장 찍기(Continuous)로

기존 LLM은 3만~10만 개의 단어장(Vocabulary)에서 하나를 고르는 '객관식 문제'를 풀었습니다. 하지만 CALM은 무한한 실수 공간에서 좌표를 찍는 '주관식 문제'를 풉니다.

 - 압축의 마법: 토큰 4개(K=4)를 하나의 연속 벡터로 압축합니다. 놀랍게도 이 벡터를 다시 토큰으로 풀었을 때 복원률은 99.9% 이상입니다.
 - 속도 혁명: 4개의 토큰을 1개의 벡터로 처리하니, 이론적으로 생성 단계(Step)가 1/4로 줄어듭니다. 한 걸음씩 걷던 모델이 이제 축지법을 쓰기 시작한 겁니다.

2. 가성비의 끝판왕: 더 크지만 더 가볍다

보통 모델이 커지면 느려지죠? CALM은 반대입니다. "의미적 대역폭(Semantic Bandwidth)"을 늘렸기 때문입니다.

 - 실험 결과: CALM-M (371M 파라미터) vs Transformer-S (281M 파라미터)
 - 덩치: CALM이 파라미터가 더 많습니다.
 - 연산량: 그런데 추론 연산량(FLOPs)은 오히려 34% 더 적습니다.
 - 성능: 그러면서도 성능 지표(BrierLM)는 더 우수합니다.

우리들에게 희소식입니다. 덩치 큰 모델을 쓰면서도 연산 비용은 획기적으로 줄일 수 있는 길이 열린 셈입니다. 다만, 해당 방법론을 적용하려면 Pre-training 부터 새로 해야 한다는 점은 유의해야 합니다.

3. Diffusion이 아닙니다: "한 방에 그린다"

연속적인 데이터를 생성한다고 하면 보통 Diffusion을 떠올리지만, 그건 너무 느립니다. 저자들은 Energy Transformer를 사용해 단 한 번의 단계(Single-step) 만에 고품질 벡터를 뽑아냅니다.

 - Diffusion: 수십 번 덧칠해서 그림 완성 (느림)
 - CALM: 붓질 한 번에 완성 (빠름)

4. 잃어버린 도구 상자: "확률이 사라졌다" (Likelihood-Free)

물론 공짜 점심은 없습니다. 이산적인 단어(Token) 대신 연속적인 벡터를 다루다 보니, LLM의 핵심인 확률(Probability)과 Softmax가 사라졌습니다.

 - 난관: 확률이 없으니 기존의 강화학습(RLHF)도, 지식 증류(Distillation)도, 심지어 평가 지표인 Perplexity(PPL)도 계산할 수 없습니다. vLLM 같은 최적화 도구도 당장은 호환되지 않습니다.
 - 해결: 저자들은 이를 위해 확률 없이도 작동하는 새로운 평가 지표(BrierLM)와, 수학적으로 완벽하게 온도를 조절하는(Temperature Sampling) 새로운 알고리즘을 직접 창시했습니다.

💡 마치며: vLLM의 데자뷔

이 논문을 보며 초창기 vLLM이 떠올랐습니다. 처음엔 호환성도 나쁘고 쓰기 불편했지만, 압도적인 속도 하나로 결국 표준이 되었죠.

CALM도 지금은 기존 생태계와 맞지 않아 불편합니다. 하지만 K=4를 넘어 K=8, K=16까지 확장된다면? 남들이 16번 연산할 때 혼자 1번 연산하는 효율을 당해낼 재간은 없을 겁니다.

우리는 지금 LLM이 글자를 읽는 시대에서 의미 덩어리를 다루는 시대로 넘어가는 변곡점을 목격하고 있는지도 모릅니다.
123




