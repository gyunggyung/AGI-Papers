---
id: 14
category: Projects
title: 현대 인공지능(LLM)은 2018년에 나온 비효율적인 GPT(트랜스포머 디코더) 구조에서 벗어나지 못하고 있습니다. 그럼에도 불구하고 Liquid AI, Technology Innovation Institute, Fastino Labs, NVIDIA 그리고 저 같은 사람들은 이 한계를 넘으려고 하고 있습니다.
---
현대 인공지능(LLM)은 2018년에 나온 비효율적인 GPT(트랜스포머 디코더) 구조에서 벗어나지 못하고 있습니다. 그럼에도 불구하고 Liquid AI, Technology Innovation Institute, Fastino Labs, NVIDIA 그리고 저 같은 사람들은 이 한계를 넘으려고 하고 있습니다.

단순한 O(N²)의 트랜스포머를 넘어서, 더 효율적인 모델을 만드는 5가지 접근법과 제 프로젝트들을 소개합니다.

Beyond Transformers:

1. Liquid AI

제가 요즘 가장 좋아하는 회사입니다. 솔직히 말이 필요한가? 싶기도 합니다.

MIT CSAIL에서 시작해 신경망의 미분방정식을 다시 쓴 이들은, 1B급 모델로 특정 테스크에서 Claude 3.5 Sonnet를 압도하는 성능을 냅니다.

 - 체급을 무시하는 지능: IFBench 점수 47.33. 1.2B 모델임에도 Claude 3.5 Sonnet(42.3)을 넘어서며, Gemini 2.5 Pro와 비견됩니다.
 - 비현실적인 속도: 갤럭시 S25 Ultra에서 70 t/s, ROG Phone 9(NPU)에서는 무려 182 t/s를 기록했습니다. 사람이 읽는 속도보다 10배 이상 빠릅니다.
 - 근본이 다른 DNA: 트랜스포머의 무거운 짐을 CNN(LIV Conv)이 짊어지게 했습니다. 1.2B 모델은 총 16개 블록 중 10개는 Conv(근육), 6개만 Attention(뇌)으로 배분해 메모리 폭발을 막았습니다.

이 모델이 너무 마음에 들어서 저는 LFM-Scholar(논문 관련 연구 자동 작성), LFM 한국어 영어 번역기 등을 직접 구현했습니다. 작은 고추가 맵다는 걸 기술적으로 증명한 팀입니다.


2. Technology Innovation Institute (Falcon)

과거 팔콘 180B를 만들어서 유명세를 얻었던 팀입니다. 이번엔 "작게 만들었다"가 아니라 "뇌의 구조를 기형적으로 비틀었다"에 가까운 모델을 가져왔습니다.

불과 몇 년 전 LLM은 14B 정도는 돼야 쓸 수 있다고 느꼈는데, 2026년 1월, Falcon-H1-Tiny 시리즈(90M, 600M)가 이 통념을 정면으로 반박했습니다.

 - Extreme Deep & Narrow: 뇌의 용량(Width)을 포기하고, 깊이(Depth)를 늘렸습니다. 90M 모델이 무려 50개의 레이어를 쌓았습니다. 13B~30B 모델에서나 볼 수 있는 깊이입니다.
 - Anti-curriculum: 모델의 '망각 창'을 역이용해 고품질 데이터를 100회 이상 반복 주입했습니다.
 - 성능: 90M 모델이 Llama-3.2-1B보다 지시 이행을 잘하고, 3배 큰 Gemma-270M보다 수학 점수가 2.5배 높습니다.

지식(MMLU)은 좀 부족해도, 시키는 일을 빠릿하게 처리하는 에이전트용으론 현존 최고의 가성비입니다.


3. Fastino

이 회사를 말씀드리고 싶어서 이 글을 쓴 측면도 있습니다. 

코딩이나 분류 업무의 경우는 디코더보다 인코더 구조가 더 적합합니다. CPU 환경에서는 특히 더 그렇습니다.

이들은 GLiNER 모델을 통해 전통적인 NER(개체명 인식) 작업을 혁신했습니다. 거대 LLM보다 수백 배 가볍지만, ChatGPT나 미세 조정된 거대 모델보다 특정 테스크에서 더 정확하고 빠릅니다. 무조건 거대 모델을 쓰는 게 능사가 아니라는 점을 보여주는 멋진 곳입니다.

하지만 제가 더 많은 정보 조사를 못해서 짧게 넘어가겠습니다. 추후에 이곳에 대해서는 다시 리뷰를 하겠습니다.


4. 엔비디아 (Nvidia)

가장 쓸만하지만, CPU 환경이 아닌 GPU 환경에 최적화된 모델이라고 생각합니다. 하지만 어느 정도의 GPU가 있거나, 램만 받쳐준다면 최선의 선택입니다.

Nemotron-3-Nano-30B-A3B. 이 모델은 Qwen3-30B보다 3.3배 빠릅니다. 단순히 속도만 빠른 게 아닙니다.

 - 맘바(Mamba) 샌드위치: 전체 52개 레이어 중 88%(46개)를 Mamba-2로 채워 O(N) 선형 시간을 확보하고, 단 6개의 Attention만 사용하여 정확도를 잡았습니다.
 - 로컬 구동: 30B 모델임에도 시스템 램 32GB만 있다면 CPU만으로 초당 20토큰(20 t/s) 이상을 뽑아냅니다.
 - 데이터 공개: 25T(25조) 토큰 분량의 학습 데이터 레시피와 리워드 모델까지 공개했습니다.

로컬 LLM을 구축하거나 RAG 시스템을 기획 중이라면, 이보다 더 완벽한 육각형 모델은 없습니다.


5. 저

저는 더 작은 모델이 특정 분야에서 큰 모델을 이길 수 있다고 믿습니다. 그렇게 특화 모델을 만들고 이들을 조율해서 MoA로 구현한다면, 로컬 16G RAM 노트북이나 라즈베리 파이에서도 충분한 작품을 만들 수 있다고 믿습니다.

하지만 자원이 부족합니다. 그래서 저는 기존 구조들의 장점만 섞어 제약 사항을 돌파하려 합니다.

[Project HybriKo: 페라리 엔진을 얹은 경운기]
NVIDIA-Nemotron 구조를 기반으로 Falcon-H1-Tiny의 깊이를 더해, 52 레이어 깊이를 불과 Active 166M 파라미터 사이즈에 압축해 넣었습니다. 동시에 지식을 넣기 위해 딥시크의 Engram도 추가했습니다. 이름하여 HybriKo-430M-A166M-Engram입니다.

아키텍처는 2026년 최신 기술의 집합체지만, 현실은 Colab T4 한 장이라 학습이 불가능합니다.

[Tiny MoA: 로컬 에이전트]
시간당 100달러 태우는 AI 에이전트들, 지속 가능할까요? 저는 반대로 갔습니다.

Liquid AI(사령관)와 Falcon(수학/코딩 전문가)을 연결해, VRAM 0MB 환경(CPU Only)에서 돌아가는 에이전트 팀을 만들었습니다. 돈 한 푼 안 들지만, 제 노트북 안에서 그럴싸하게 협업합니다.


마치며

저는 효율적인 온디바이스 LLM이 필요하다고 생각합니다. 보안, 통제력, 비용 측면에서 필수적입니다. 거대 테크 기업들이 API 가격을 올리거나 기술을 비공개로 전환한다면(중국은 이미 시작했습니다), 우리에겐 우리가 통제할 수 있는 모델이 있어야 합니다.
141




