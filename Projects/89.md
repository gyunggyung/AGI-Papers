---
id: 89
category: Projects
title: LFM2 1.2B 한국어 영어 번역기 제작
---
# LFM2 1.2B 한국어 영어 번역기 제작

2019년, OpenAI는 GPT-2(1.5B)를 발표하며 "인류에게 너무 위험할 수 있다(Too dangerous to release)"는 이유로 전체 모델 공개를 거부했었습니다.

그로부터 6년이 지난 지금, 저는 그때보다 작은 1.2B(Liquid AI LFM2) 모델을 Colab과 Kaggle의 무료 GPU(T4) 환경에서 튜닝했습니다. 결과는 흥미롭습니다. 이 위험할 뻔했던 체급의 모델이 구글과 알리바바의 최신 4B 모델들을 번역 성능에서 앞섰습니다.

## 1. 정량적 성능 평가 (Quantitative Evaluation)

*   **LFM2-KoEn-v5-RL (1.2B):** CHrF++ 32.96 / BLEU 12.05
*   **Gemma-3-4B (4B):** CHrF++ 32.83 / BLEU 11.36
*   **Qwen3-4B (4B):** CHrF++ 25.62 / BLEU 7.46

**👉 Result:**
불과 1.2B 파라미터로 3배 이상 거대한 모델들을 제쳤습니다.

*   **vs Gemma-3 (4B):** 구글의 최신 모델보다 CHrF++ 0.13 포인트 앞섭니다.
*   **vs Qwen3 (4B Base):** 알리바바의 강력한 베이스 모델보다 무려 CHrF++ 7.3 포인트 이상 높은 압도적인 성능 격차를 보여줍니다. (추후 Instruct 모델 테스트 예정)

이는 모델의 크기(Size)보다 도메인 특화 데이터와 학습 전략(Specialization)이 실질적인 성능을 결정함을 시사합니다.

## 2. 학습 전략 (Methodology)

LFM2의 기본 한국어 능력 잠재력을 번역 도메인에서 극대화하기 위해, 고품질 데이터 SFT Full Fine-tuning 후 GRPO 강화학습을 결합하는 전략을 사용했습니다.

*   **Step 1: SFT (Supervised Fine-tuning):** 10만 개의 고품질 한영 병렬 데이터셋으로 베이스를 다졌습니다. 이 과정에서 Maxime Labonne 님의 조언이 결정적이었습니다. 덕분에 베이스라인 대비 CHrF++ 점수를 4.3점 이상 끌어올렸습니다. (Special thanks to Maxime!)
*   **Step 2: RL (Reinforcement Learning):** SFT 모델을 기반으로 1만 개의 추가 데이터를 투입, GRPO(Generative Reinforcement Policy Optimization)를 적용하여 LoRA 어댑터를 훈련했습니다. 이를 통해 SFT 모델 대비 약 1.5점의 추가 성능 향상을 이끌어냈습니다.

## 3. 한계 및 향후 과제 (Limitations & Future Work)

GPU 자원과 시간의 한계로 Full Parameter RL 학습이나 다양한 양자화(Quantization) 비교 실험은 아직 완료하지 못했습니다.

현재 20만 개의 SFT 데이터와 2.5만 개의 RL 데이터를 확보했습니다. 추후, 최근 리뷰한 Qwen 팀의 논문("Stabilizing RL with LLMs") 내용을 검증할 계획입니다. 논문에서 제안한 대로 GRPO의 '길이 정규화(Length Normalization)' 등 불필요한 옵션을 제거했을 때, 수학적 단순함이 실제 성능 극대화로 이어지는지 테스트해보려 합니다.

과거에는 공개할 수도 없는 '위험한 크기'의 모델이, 이제는 누구나 로컬에서 돌릴 수 있는 '효율적인 도구'가 되었습니다.
