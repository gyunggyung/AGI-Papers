---
id: 105
category: Post_Training
title: Preference Leakage: A Contamination Problem in LLM-as-a-Judge
---
Preference Leakage: A Contamination Problem in LLM-as-a-Judge

1. 배경 및 문제 정의

LLM-as-a-Judge의 평가 문제

최근 LLM을 활용한 모델 평가 방식인 LLM-as-a-Judge가 활성화됨.
그러나 이 방법은 편향(Bias) 문제가 있으며, 대표적으로:
길이 편향(Length Bias): 긴 답변을 더 선호하는 경향.
자기 중심 편향(Egocentric Bias): LLM이 자신의 출력을 더 선호하는 문제.

이 논문에서는 새로운 평가 편향 문제인 **Preference Leakage(선호도 유출)**을 정의함.

Preference Leakage란?
LLM이 데이터 생성과 평가를 동시에 수행할 때 발생하는 평가 오염 문제.
데이터 생성 모델과 평가 모델이 연관이 있을 경우, 평가 모델이 자신과 관련된 모델의 출력을 더 선호하는 현상이 발생.
이는 기존의 데이터 오염(Data Contamination) 문제와 유사하지만, 더 은밀하게 발생하며 검출하기 어려움.

2. Preference Leakage의 주요 원인

논문에서는 LLM 데이터 생성기(Generator)와 평가자(Judge) 간의 **세 가지 유형의 관련성(Relatedness)**을 정의함.

1. 동일한 모델(Same Model)
데이터 생성과 평가를 같은 LLM이 수행. (예: GPT-4가 생성하고 GPT-4가 평가)

2. 계승 관계(Inheritance Relationship)
데이터 생성 모델이 평가 모델에서 파생됨. (예: GPT-4 기반으로 학습된 모델을 GPT-4가 평가)

3. 같은 모델 계열(Same Model Family)
동일한 모델 패밀리 내에서 데이터 생성과 평가가 이루어짐. (예: GPT-4가 생성하고 GPT-3.5가 평가)

Preference Leakage가 발생하는 이유
데이터 생성기와 평가 모델이 같은 특성을 공유하기 때문. 데이터 생성 과정에서 특정 스타일, 형식, 표현 방식이 반영되며, 평가 모델이 이를 인식하고 선호하는 경향이 발생. 특히 Supervised Fine-Tuning(SFT) 과정에서 이러한 편향이 더 심해짐.

3. 실험 및 주요 결과

실험 결과

1. Preference Leakage는 실제로 발생하며, 심각한 문제임.
대부분의 모델 조합에서 평가 모델이 자신의 관련 모델을 더 선호하는 경향을 보임.

2. 모델의 크기와 성능이 높을수록 Preference Leakage가 더 심해짐.
Qwen-2.5-14B (더 큰 모델)이 Mistral-7B보다 더 높은 Preference Leakage Score를 기록. 성능이 좋은 모델이 더 강하게 패턴을 학습하기 때문으로 추정됨.

3. Supervised Fine-Tuning(SFT)이 가장 큰 Preference Leakage를 유발함.
SFT 방식은 23.6%로 가장 높았으나, **DPO(Direct Preference Optimization)**는 5.2%, **ICL(In-Context Learning)**은 -2.7%로 훨씬 낮음.

4. 데이터 혼합(Data Mixing) 전략이 Preference Leakage에 영향을 줌.
합성 데이터(Synthetic Data) 비율이 높을수록 Preference Leakage Score가 증가. 70% 합성 데이터 포함 시 최대 30%까지 증가.

5. 평가 모델은 자신의 학생 모델 출력을 정확히 인식하지 못하지만, 특정 패턴을 학습함.
LLM 평가자는 학생 모델의 출력을 직접 식별하는 능력이 낮았지만, BERT 기반 분류 모델은 높은 정확도로 구분 가능함.

4. 해결 방법 및 시사점

해결 방안
1. 평가 모델과 데이터 생성 모델을 분리해야 함.
2. Supervised Fine-Tuning(SFT)보다 DPO, ICL 같은 학습 방법 사용.
3. 합성 데이터의 비율을 줄이고, 다양한 모델에서 생성한 데이터를 혼합.
4. 새로운 평가 벤치마크 개발.

시사점
LLM-as-a-Judge는 비용 효율적이지만, 모델 편향(Bias) 문제를 해결하지 않으면 신뢰할 수 없음. 특히 Preference Leakage는 기존의 데이터 오염보다 더 은밀하고 검출하기 어려움. 평가 모델의 공정성을 확보하려면 데이터 생성과 평가 모델을 분리하고, 새로운 학습 기법과 평가 기준을 개발해야 함.
