---
id: 58
category: Architecture
title: 새벽 4시, Liquid AI의 LFM2-2.6B-Exp를 튜닝해서 논문의 Related Work 섹션을 통째로 생성하는 도구를 만들었습니다.
---
새벽 4시, Liquid AI의 LFM2-2.6B-Exp를 튜닝해서 논문의 Related Work 섹션을 통째로 생성하는 도구를 만들었습니다.

Step-DeepResearch 논문(32B 모델)을 리뷰하다 문득 이런 생각이 들었습니다. "32B가 아니라 2.6B로도 되지 않을까?" (해당 논문 리뷰는 추후 작성 예정)

그러다 Kyudan Jung님의 citeAgent가 떠올랐습니다. 논문 작성 중 BibTeX를 자동 생성해주는 멋진 도구였죠. 하지만 저는 LLM API를 쓰기 싫었습니다. 아이디어가 유출되는 느낌이 있죠. 그래서 새벽에 3-4시간 바이브 코딩을 해서 LFM-Scholar를 만들었습니다.

🔒 핵심 가치: Local-First

🧠 왜 2.6B인가?
Liquid AI의 LFM2-2.6B-Exp는 지시이행 능력이 R1보다 낫다고 합니다. 이미 잘하는 모델이니 LoRA로 소량의 데이터만 학습하면 될 거라 생각했습니다. 결과는 놀라웠습니다. 대충 써도 잘하고, 실제 논문 Abstract을 줘도 잘 작동했습니다.

📱 노트북에서, 심지어 핸드폰에서도
2.6B는 아무 노트북에서나 실행됩니다. 심지어 모바일에서도 돌릴 수 있는 크기입니다. GPU 없이도 연구 보조 도구를 쓸 수 있다는 건 꽤 의미있는 일이라고 생각합니다.

🔍 기술적 접근
- Multi-API Fallback: Semantic Scholar → OpenAlex → arXiv 3단계 자동 전환
- Smart Multi-Query: 모델명+연도, 약어(RNN, LSTM 등) 자동 인식
- Hallucination Detection: 의심스러운 인용 감지 및 경고

⚠️ 한계

🙏 감사
이 프로젝트는 Kyudan Jung님의 citeAgent에서 영감을 받았고, Step-DeepResearch 논문을 읽으며 작은 모델로도 학술 연구 보조가 가능하다는 확신을 얻었습니다. Liquid AI의 LFM2-2.6B-Exp 덕분에 빠르게 좋은 모델을 만들 수 있었습니다.


🔗 GitHub: https://lnkd.in/gF_Pa2yS
likesupportlove
67




