---
id: 92
category: Post_Training
title: 알리바바 Qwen 팀이 LLM 강화학습(RL)에 새로운 바이블을 냈습니다. 제목은 "Stabilizing Reinforcement Learning with LLMs: Formulation and Practices". 결론부터 말하자면, 이 논문은 "GRPO나 CISPO 같은 화려한 최신 기교(Heuristics)보다, 수학적으로 올바른 기본기가 더 강력하다"는 것을 철저한 실험으로 증명합니다.
---
알리바바 Qwen 팀이 LLM 강화학습(RL)에 새로운 바이블을 냈습니다. 제목은 "Stabilizing Reinforcement Learning with LLMs: Formulation and Practices". 결론부터 말하자면, 이 논문은 "GRPO나 CISPO 같은 화려한 최신 기교(Heuristics)보다, 수학적으로 올바른 기본기가 더 강력하다"는 것을 철저한 실험으로 증명합니다. 

1. 단순함이 승리하는 이유 (Back to Basics)

우리는 보통 RL 성능을 높이려 최신 알고리즘들처럼 '길이 정규화(Length Normalization)' 같은 기교(Heuristics)를 섞곤 합니다. 하지만 논문은 이런 기교들이 오히려 독이 된다고 지적합니다.

LLM 강화학습의 상황을 '백일장 대회'에 비유할 수 있습니다. 심사위원은 글을 다 읽고 '글 전체(Sequence)'에 대해 점수를 줍니다. 하지만 우리는 더 좋은 점수를 받기 위해 문장 속의 '단어 하나하나(Token)'를 고쳐야 합니다. 

전체 글 점수만 보고 특정 단어를 바꾸는 게 맞는지 알기 어렵지만, 논문은 우리가 수행하는 토큰 단위 학습이 전체 점수의 '수학적 1차 근사(First-order Approximation)'임을 규명했습니다. 여기에 인위적인 정규화를 섞으면 이 수학적 연결고리가 깨져버립니다.

또한, "너무 옛날 기억으로 공부하지 마라(정책 노후화 방지)"라는 원칙도 중요합니다. 

축구 선수가 지금의 감각으로 슛을 차야 하는데, 축구를 처음 배울 때의 엉성한 폼을 강제로 따라 하라고 하면 오히려 골을 못 넣겠죠? 실력은 프로가 되었는데, 몸은 초보 때의 기억을 따라가려니 스텝이 꼬이는 겁니다.

모델은 계속 똑똑해지는데, 과거의 모델이 만든 낡은 데이터로 학습하면 성능이 낮아집니다. 그래서 '클리핑(Clipping)' 기술을 써서 "잠깐! 이건 너무 옛날 데이터니 반영하지 말자"라고 막아줘야 합니다.

- 실험 결과(Figure 1), 가장 성능이 좋고 안정적인 모델은 새로운 알고리즘이 아니었습니다. 'MiniRL'이라 불리는, 기본 REINFORCE 알고리즘에 딱 두 가지 안전장치만 건 모델이었습니다. (Qwen3-30B-A3B-Base 학습 결과)

 MiniRL (Winner): AIME 등 벤치마크에서 정확도 78% 수준까지 안정적으로 우상향했습니다. 

 With Length-Norm (GRPO 스타일): 학습은 진행되지만, 수학적 근사가 깨진 탓에 성능이 약 75% 수준에 머물렀습니다. (MiniRL 대비 약 -3%p 성능 저하) 

 Without IS Correction (확률 보정 미적용): 학습 환경 불일치를 보정하지 않자, 불과 150~200 스텝 만에 엔트로피가 급락하며 학습이 붕괴(Collapse)되었습니다. 

(참고: 클리핑을 끈 CISPO 스타일의 경우, 오프-폴리시 설정에서 학습 후반부에 급격한 불안정성을 보이며 최고 성능 도달 전 60% 대로 추락하거나 붕괴했습니다.)

2. MoE 모델을 위한 필수 조건: "기억을 고정하라"


논문은 이를 해결하기 위해 '라우팅 리플레이(Routing Replay)'를 제안합니다. 추론 시점의 전문가 경로를 기억해뒀다가, 학습 때 강제로 똑같은 길을 가게 만드는 것입니다.

실험 결과 (Figure 3, 4): 배치 사이즈를 키운 오프-폴리시(Off-policy) 설정에서,

 라우팅 리플레이 미적용(No R3): 학습 시작 직후 성능이 60% 이하로 곤두박질치며 회복하지 못했습니다. 

 라우팅 리플레이 적용(With R3): Dense 모델처럼 안정적인 곡선을 그리며 최종 정확도 78%에 도달했습니다. 

MoE에서 RL을 하려면 이 장치는 선택이 아닌 필수입니다.

3. 훌륭한 교사보다 중요한 건 '끈기 있는 학습'

"초기 모델(SFT)이 얼마나 좋아야 할까?"에 대한 흥미로운 실험도 있습니다. Qwen3-Max, DeepSeek-R1, GPT-OSS 등 서로 성능이 다른 교사 모델로 초기화를 했음에도, 결과는 놀라웠습니다.

 실험 결과 (Figure 5): 시작점은 서로 달랐지만, 안정적인 RL을 충분히 길게 수행하자(Prolonged Optimization) 모든 모델이 AIME 벤치마크 0.86~0.88 (86~88%) 구간으로 수렴했습니다. 

초기값(SFT)의 차이는 RL 과정에서 씻겨 내려갑니다. 중요한 건 "어디서 시작하느냐"보다 "얼마나 안정적으로 끝까지 학습시킬 수 있느냐"였습니다.

4. 물론, 한계는 있다 (Student Limits)

그렇다고 소형 모델이 RL만으로 400B 모델을 이길 수 있을까요? 그건 아닙니다. RL은 학생이 가진 잠재력(Capacity)의 최대치(Peak)를 찍게 해줄 뿐, 학생의 체급 자체를 바꿔주진 못합니다. 교사 모델이 애초에 모르는 영역(Unknowns)은 학생도 배울 수 없다는 물리적 한계는 명확했습니다.

💡 마치며: 왜 이 논문을 읽어야 하는가

이 논문은 단순히 "성능이 좋다"를 넘어, 우리가 관습적으로 사용해온 RL 알고리즘들이 왜 수학적으로 정당한지, 그리고 어떤 조건이 깨지면 알고리즘이 무너지는지를 수식으로 대변해 줍니다.

156

love
마음에 쏙듬



