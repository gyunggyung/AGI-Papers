---
id: 33
category: Architecture
title: 우리들이 환호할 만한, 거대 모델의 지식을 로컬에서 돌리는 가장 현실적인 방법을 6가지 핵심으로 정리했습니다.
---
우리들이 환호할 만한, 거대 모델의 지식을 로컬에서 돌리는 가장 현실적인 방법을 6가지 핵심으로 정리했습니다.

1. 문제 의식: "왜 아는 걸 매번 계산해서 끄집어내는가?"

논문은 FFN 레이어를 '키-값 메모리(Key-Value Memory)'로 정의합니다. 기존 모델은 지식을 꺼낼 때마다 비싼 행렬 연산을 수행했습니다.

 - Width (지식): "이순신이 누구야?" 같은 사실적 지식. 기존에는 Up-projection 행렬이 담당했습니다.
 - Depth (사고): 문맥에 따라 정보의 중요도를 조절하는 기능. 이는 Gate가 담당합니다.

STEM은 "단순 지식(Up-projection)을 꺼내는 데 비싼 GPU 연산을 쓰지 말고, 그냥 임베딩 테이블에서 찾아오면(Lookup) 된다"고 제안합니다.

2. 구현: FFN의 수술 (행렬을 테이블로 교체)

기존 FFN은 Up-projection, Gate, Down-projection 세 가지 행렬로 구성됩니다. STEM은 여기서 지식 확장을 담당하는 Up-projection 행렬을 제거하고, 그 자리에 거대한 토큰 임베딩 테이블을 심었습니다.

 - 기존: 입력 x 행렬곱 = 지식 추출 (GPU 연산 소모)
 - STEM: 입력 토큰 ID 보고 테이블에서 값 가져오기 (단순 조회)

놀라운 점은 이 거대한 지식 테이블을 GPU 메모리(VRAM)가 아닌, 값싼 CPU 메모리(RAM)에 둬도 된다는 겁니다. GPU는 사고(Gating)만 하고, 지식은 CPU에서 비동기로 가져옵니다.

중요한 건 문맥을 파악하는 Gate와 정보를 압축하는 Down-projection은 그대로 둬서(Dense), 모델의 사고력(Reasoning)은 유지했다는 점입니다.

3. 효율성: 다이어트했는데 뇌 용량은 커졌다

이 구조적 변화로 FFN 레이어의 파라미터 약 1/3을 제거했습니다. 그런데 성능은 오히려 올랐습니다.

 - 연산량(FLOPs): FFN 연산량과 파라미터 로딩 비용이 약 1/3로 감소했습니다.
 - 성능: 놀랍게도 ARC-Challenge, OpenBookQA 같은 지식 집약적 태스크에서 Dense 모델보다 약 9~10% 더 높은 성능을 기록했습니다.
 - 시스템: 지식 테이블을 CPU로 오프로딩하여 VRAM 사용량을 획기적으로 줄였습니다.

4. 해석 가능성: "스페인의 수도는 베를린?"

STEM의 가장 흥미로운 실험입니다. 모델에게 "스페인(Spain)"에 대해 물어보면서, 내부적으로 '스페인' 토큰의 임베딩 벡터만 몰래 '독일(Germany)' 벡터로 바꿔치기했습니다.

그랬더니 모델은 아무렇지 않게 "베를린은 스페인의 수도이며..."라고 글을 써 내려갑니다.

이는 LLM이 창조자가 아니라, 주어진 재료(벡터)를 받아 문법에 맞게 조립(Organize)하는 기계임을 증명합니다. 이를 역이용하면, 재학습 없이 특정 지식만 수술하듯 교체(Editing)할 수 있습니다.

5. 긴 문맥(Long Context): 쓸수록 강해진다

MoE 모델은 문맥이 길어지면 전문가 호출이 늘어나 효율이 떨어지지만, STEM은 반대입니다.


6. DeepSeek Engram과의 관계: "최강의 듀오"

지난번 소개한 DeepSeek AI의 Engram과 이번 STEM은 서로 경쟁자가 아닙니다. 이론상 같이 썼을 때 완벽한 시너지를 낼 수 있습니다.

 - Engram (초반 레이어): N-gram 단위의 문맥을 파악해 CPU에서 가져옵니다.
 - STEM (중/후반 레이어): 심층 지식을 CPU 테이블에서 가져옵니다.
 - GPU (Core): 위에서 가져온 재료들로 논리적인 추론만 수행합니다.

이 하이브리드 구조야말로 100M~1B 급의 작은 모델로도 거대 모델의 지식을 다루는 On-Device AI의 새로운 형태가 아닐까 싶습니다.


우리는 모델을 처음부터 학습(Pre-training)할 자원이 부족합니다. 하지만 STEM은 희소식입니다.

1. Mid-training: 논문에서는 기존 모델을 가져와 STEM으로 교체 후 추가 학습(Mid-training)해도 성능이 향상됨을 증명했습니다.
2. CPU Offloading: 지식은 램(RAM)에 담고, 사고는 칩(GPU)으로 하는 구조.

77




