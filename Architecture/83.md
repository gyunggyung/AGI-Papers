---
id: 83
category: Architecture
title: Diffusion LLM (100B Parameters)
---
# Diffusion LLM (100B Parameters)

100B 파라미터의 Diffusion LLM 모델이 나왔습니다. 그런데 30B 모델보다 2배 이상 빠릅니다. 그동안 Diffusion LLM은 병렬 생성이라는 잠재력 덕분에 기대를 모았지만, 바닥부터 학습(From Scratch)해야 해서 비용이 너무 비싸고, 모델이 커질수록 학습이 불안정해져 100B 급의 대형 모델은 전무했죠.

Ant Group의 InclusionAI팀이 발표한 LLaDA2.0은 이 문제를 영리하게 풀었습니다. "기존 AR(Auto-regressive) 모델을 가져와서, Diffusion 모델로 개조하는 레시피"를 제시한 겁니다.

핵심적인 구현 디테일과 한계점을 5가지로 정리했습니다.

## 1. 체급의 비밀: 6B(Active)가 3B(Active)를 속도로 이기다

이 논문의 비교 대상인 `Qwen3-30B-A3B`는 이름처럼 Active Parameter가 약 3B인 가벼운 MoE 모델입니다. 반면 `LLaDA2.0-flash`는 총 100B, Active 약 6B인 MoE 모델입니다.

*   **상황:** LLaDA가 매 스텝마다 연산해야 할 파라미터가 2배 이상 많습니다(6B vs 3B). 짐이 훨씬 무거운 셈입니다.
*   **결과:** 그런데 추론 속도(TPS)는 LLaDA(535)가 Qwen(237)보다 2.2배 빠릅니다.
*   **이유:** AR 모델은 가벼운 짐(3B)을 들고 100번 왔다 갔다 해야 문장을 만드는데, Diffusion 모델은 조금 무거운 짐(6B)을 들고 20~30번만 큼직하게 움직여서(Block Diffusion) 문장을 통째로 완성해버리기 때문입니다.

> **참고:** 베이스 모델인 Ling 100B(256 TPS)의 속도가 이미 Qwen3-30B-A3B와 비슷합니다. 하지만 LLaDA는 최적화를 통해 베이스 모델 대비 2배 이상의 속도 향상을 이뤄냈습니다.

## 2. Warmup: AR의 뇌를 속이는 "빈칸 채우기" 놀이

이 모델은 100B를 처음부터 굽지 않았습니다. `Ling`이라는 AR 모델을 개조했습니다. 문제는 AR 모델은 평생 앞만 보고 달린 녀석이라, 갑자기 전체를 보라고 하면 멍해진다는 겁니다. 저자들은 이를 WSD(Warmup-Stable-Decay) 전략으로 해결했습니다.

여기서 핵심은 "블록 크기 1"로 초기화하는 것입니다. 아주 쉬운 비유가 있습니다.

*   **AR (기존):** "나는 학교에 [?]" -> "간다" (다음을 예측해!)
*   **Diffusion (블록=1):** "나는 학교에 [MASK]" -> "간다" (빈칸을 채워!)

모델 입장에서는 "다음을 맞추나, 빈칸을 채우나" 뇌를 쓰는 방식은 똑같습니다.
저자들은 이 점을 이용해, 처음에는 단어 하나짜리 빈칸 채우기(Block=1)로 AR 모델을 안심시킨 뒤, 점차 빈칸의 크기를 문장 단위, 문단 단위로 늘려가며(Warmup) 자연스럽게 Diffusion 모델로 진화시켰습니다.

*   **Warmup:** 블록 크기 1 → 4096. 문맥 전체를 보는 눈을 뜨게 함.
*   **Stable:** 전체 시퀀스를 마스킹하고 복원하며 성능 극대화.
*   **Decay:** 추론 속도를 위해 다시 블록 크기를 줄여(4096 → 32), KV-Cache 효율과 병렬성 확보.

## 3. 초기화의 마법: 노이즈 주입 (Gaussian Noise Injection)

가장 인상적인 엔지니어링 디테일입니다. AR 모델을 가져와서 Diffusion으로 돌릴 때 가장 큰 문제는 `[MASK]` 토큰입니다.

AR 모델은 평생 `[MASK]`라는 토큰을 본 적이 없습니다. 그래서 임베딩 값이 0이거나 초기화가 안 되어 있어, 학습 시작 즉시 그래디언트 폭발(Gradient Explosion)이 일어납니다.

저자들은 이를 막기 위해 초기 학습 단계에서 `[MASK]` 토큰 임베딩에 가우시안 노이즈를 강제로 주입했습니다. "이 토큰은 낯설겠지만 일단 처리해 봐"라고 모델을 달래가며, 치명적 망각(Catastrophic Forgetting) 없이 자연스럽게 Diffusion 메커니즘을 이식시켰습니다.

## 4. 성능: 암기를 버리고 사고를 얻다

AR에서 Diffusion으로 바꾸면 무엇이 달라질까요? 성능표(Table 2)가 명확한 트레이드오프를 보여줍니다.

*   **상승 (Reasoning & Coding):** 코딩(HumanEval 94.51점)과 에이전트 능력에서는 베이스 모델인 AR보다 성능이 좋습니다. Diffusion 특유의 양방향 문맥 참조가 전체적인 구조 설계(Planning)에 유리하기 때문입니다.
*   **하락 (Knowledge):** 반면 단순 지식 인출(MMLU, GPQA) 능력은 소폭 하락했습니다. '다음 단어를 정확히 맞추는' AR의 칼 같은 정밀함보다는, '전체 의미를 복원하는' 근사적 접근의 한계로 보입니다.

## 5. 라인업: 작고 빠른 놈, 크고 똑똑한 놈

이 레시피가 범용적이라는 증거로 두 가지 체급을 공개했습니다.

*   **LLaDA2.0-mini (16B):** Active 1.4B 수준. Qwen3-8B와 경쟁하며 코딩에 강점.
*   **LLaDA2.0-flash (100B):** 위에서 언급한 고성능 모델.

---

## 💡 마치며: 다만 아쉬운 점

정말 좋은 논문이고 모델 가중치와 학습 코드(GitHub)까지 공개한 점은 훌륭합니다. 하지만 "데이터 레시피"가 빠져 있습니다. 이 정도 성능을 내기 위해 어떤 데이터를 얼마나(토큰 수) 쏟아부어야 하는지 명확하지 않습니다. 100B 모델을 CPT(Continual Pre-training) 하는 것만으로도 상당한 GPU 자원이 필요할 텐데 말이죠.

또한 성능에 대한 의문도 남습니다. 100A6B(Active 6B) 스펙인데, 30A3B(Active 3B)인 Qwen 모델과 성능이 비슷합니다. 물론 베이스 모델(Ling)의 성능을 따라가는 것이니 어쩔 수 없지만, 비슷한 체급인 OpenAI의 GPT-OSS-120B(Active 5.1B) 등과 비교하면 격차가 클 것으로 보입니다.

더 상위 체급의 베이스 모델에서도 이 확산(Diffusion) 방식이 성능 유지를 해낼 수 있을지, 추가적인 검증이 필요해 보입니다.
