---
id: 60
category: Pre_Training
title: Python 재귀로 시작하는 1,000만 토큰 시대
---
Python 재귀로 시작하는 1,000만 토큰 시대


2025년 12월 31일, MIT CSAIL 팀이 발표한 Recursive Language Models (RLMs)는 이 문제를 하드웨어(VRAM)가 아닌 소프트웨어(Algorithm)로 돌파했습니다.

단순히 읽는 창을 늘리는 것이 아니라 읽는 방식을 혁신하여, 1,000만 토큰 이상의 입력을 처리하면서도 성능은 1000배 올리고 비용은 60% 줄인 핵심 디테일 5가지를 정리했습니다.

1. 패러다임의 역전: 프롬프트는 입력이 아니라 환경이다

이 논문의 가장 큰 도약은 텍스트를 LLM에게 억지로 먹이지 않는다는 점입니다. 대신 Python REPL 환경의 변수(Variable)로 저장합니다.

 - 기존 방식 (In-context): 1,000페이지 책을 입력창에 욱여넣음 → OOM 발생, 앞 내용 망각.
 - RLM 방식 (Environment): book = load(war_and_peace.txt) 처럼 변수에 할당해 둠.

이제 LLM은 텍스트를 읽는 독자가 아니라, 데이터에 접근하는 프로그래머가 됩니다. 3챕터 읽어줘가 아니라 print(book[10000:20000]) 코드를 짜서 필요한 데이터만 메모리에 로드(Fetch)합니다.


2. 메커니즘: 분신술을 쓰는 작업반장 (Recursion)

이름이 왜 재귀(Recursive)일까요? 단순히 for 루프를 도는 게 아니라, 모델이 생성한 코드 안에서 llm_query() 함수를 통해 자기 자신을 호출하기 때문입니다.

쉬운 비유로 하청을 주는 작업반장과 같습니다.

 - 반장(RLM): 이 100GB 로그 파일에서 에러 패턴 찾아. (혼자 처리 불가)
 - 행동: 파일을 100개로 쪼개는 코드를 짭니다. 그리고 자기 자신의 복제(Sub-call) 100명을 소환해서 각 조각을 던져줍니다.
 - 결과: 복제들이 가져온 보고서를 변수에 저장하고, 최종적으로 반장이 이를 취합(Stitching)합니다.



3. 성능: 0.04% vs 58.00% (암기를 넘어선 추론)


 - GPT-5 (Base): 0.04% (입력이 너무 길어지자 사실상 0점, 포기)
 - RLM (GPT-5): 58.00% (압도적 성능)
 - Qwen3-Coder: 베이스 모델은 실패했으나, RLM 적용 시 23.11% 달성.

모델 파라미터(Weight)는 똑같은데, 일하는 방식만 바꿨더니 불가능하던 태스크가 해결되었습니다. 이는 Inference-time Scaling(추론 시간 스케일링)이 메모리의 물리적 한계를 극복하는 열쇠임을 증명합니다.



보통 성능을 높이려면 돈을 쏟아부어야 하죠? RLM은 정반대입니다. 무식하게 전체 토큰을 연산하는 게 아니라, 코드를 통해 정말 필요한 부분만 골라 읽기(Look up) 때문입니다.

 - 실험 결과: BrowseComp+ (1K Docs) 태스크에서 GPT-5 Mini로 전체를 읽으면 $2.75가 들지만, RLM 방식을 쓰면 $0.99로 해결했습니다.
 - 비용은 64% 절감하고, 성능은 44% → 91%로 수직 상승했습니다.


5. 적용성: 오늘 당장 쓸 수 있다 (No Retraining)

이 기술의 가장 큰 매력은 재학습이 필요 없다는 점입니다. DeepSeek, Claude, Qwen, GPT 등 코딩 능력이 있는 모델이라면, 약간의 엔지니어링과 파이썬 샌드박스만 붙여서 바로 RLM으로 변신시킬 수 있습니다.

논문은 GPT-5 같은 SOTA 모델뿐만 아니라 500B 급 오픈 모델에서도 이 프레임워크가 완벽하게 작동함을 보였습니다.


💡 마치며: Inference-time Scaling의 시대

우리는 그동안 더 큰 컨텍스트, 더 많은 VRAM을 외치며 하드웨어 혹사를 시켜왔습니다. 하지만 이 논문은 소프트웨어적 접근(Code Generation + Recursion)이 훨씬 우아하고 강력함을 증명합니다.

41

love
마음에 쏙듬



