---
id: 32
category: Projects
title: "시간당 100달러 태우는 AI, 언제까지 지속 가능할까요?": Tiny MoA (CPU-Only Mixture of Agents) 요즘 트렌드는 신기합니다. 클로드 코드로 멀티 에이전트를 10개씩 돌리고, Ralph Loop 방식으로 무한 루프를 돌려 성능을 끌어올립니다. 엄청난 성능이지만, 시간당 10달러, 100달러, 심지어 5,000달러가 깨지기도 합니다. 많은 분들이 코딩을 하기 위해 매달 수 십 만원을 앤트로픽에 지불합니다. (참고로 저는 AI에 돈을 전혀 안 쓰고 있습니다)
---
"시간당 100달러 태우는 AI, 언제까지 지속 가능할까요?": Tiny MoA (CPU-Only Mixture of Agents) 요즘 트렌드는 신기합니다. 클로드 코드로 멀티 에이전트를 10개씩 돌리고, Ralph Loop 방식으로 무한 루프를 돌려 성능을 끌어올립니다. 엄청난 성능이지만, 시간당 10달러, 100달러, 심지어 5,000달러가 깨지기도 합니다. 많은 분들이 코딩을 하기 위해 매달 수 십 만원을 앤트로픽에 지불합니다. (참고로 저는 AI에 돈을 전혀 안 쓰고 있습니다)

그런데 Anthropic이나 OpenAI가 API 가격을 조금만 올려도 이 구조는 지속 가능하지 않습니다. 그래서 저는 반대로 가봤습니다. "작은 모델들이 잘하는 일은 없을까?"

가장 똑똑한 1.2B 모델(Brain)이 600M 전문가(Specialist)들을 지휘한다면, 돈 한 푼 안 드는 내 노트북 CPU에서도 에이전트 시스템이 돌아가지 않을까? 그렇게 2시간 만에 가볍게 만들어본 Tiny MoA입니다.

🎯 목표
VRAM 0MB 환경(CPU Only, RAM 16GB)에서 돌아가는 초경량 MoA 구현.

단일 모델보다 더 전문적인 답변을 하면서도, 시스템 리소스는 최소화하는 것이 목표였습니다.

🧠 아키텍처 (The Squad)
단 1.5GB 램으로 구동됩니다.

1. Brain: Liquid AI LFM2.5-1.2B (Instruct)
 - 역할: 사령관. 의도 분석, 라우팅, 최종 답변 통합, 그리고 한국어 처리.
 - 선정 이유: 1B 체급에서 Instruction Following 포함 각종 성능이 압도적입니다.

2. Specialist: Technology Innovation Institute Falcon-H1-Tiny-R-0.6B
 - 역할: 코딩(Coding) 및 수학(Math) 담당.
 - 선정 이유: 600M인데 LiveCodeBench 39%, AIME25 67.3를 찍은 작은 거인입니다.

🛠️ 작동 방식
사용자가 "피보나치 수열 파이썬으로 짜줘"라고 물으면:
1. Brain이 "이건 코딩 문제다"라고 판단 (Routing)
2. Brain이 Falcon(Specialist)을 깨워서(Lazy Loading) 일을 시킴
3. Falcon이 짠 코드를 받아 Brain이 우아하게 답변 출력

이 모든 과정이 인터넷 연결 없이, 로컬 노트북 CPU에서 끊김 없이 돌아갑니다.

📉 한계와 가능성


GitHub에 전부 공개했습니다. 로컬 AI의 극한 효율을 경험해보고 싶으신 분들은 한번 돌려보세요. 그리고 추후에는 제 모델도 추가해서 더 다양한 기능을 구현할 계획입니다.

🔗 GitHub: https://lnkd.in/gmY8afKq


likelovesupport
61




