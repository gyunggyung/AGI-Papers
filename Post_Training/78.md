---
id: 78
category: Post_Training
title: Yann LeCun은 최근 자신의 포스트를 통해 "나는 LLM을 포함한 생성형 아키텍처가 물리적 세계의 작동 방식을 배울 수 없다는 것을 알고 있었다."고 비판하며, 진정한 지능은 월드 모델(World Model)을 통한 이해와 계획에 있다고 재차 강조했습니다.
---
Yann LeCun은 최근 자신의 포스트를 통해 "나는 LLM을 포함한 생성형 아키텍처가 물리적 세계의 작동 방식을 배울 수 없다는 것을 알고 있었다."고 비판하며, 진정한 지능은 월드 모델(World Model)을 통한 이해와 계획에 있다고 재차 강조했습니다.


하지만 정답지가 없는 번역이나 작문 영역에서 어떻게 모순을 측정할 수 있을까요? 이 난제를 '일반화된 쌍대성(Generalized Duality)'으로 풀어낸 논문이 나왔습니다. ByteDance의 DuPO입니다.

DuPO: Enabling Reliable LLM Self-Verification via Dual Preference Optimization은 르쿤의 이상(에너지 최소화)을 현재의 LLM 구조 안에서 가장 현실적이고 효율적으로 구현해냈습니다.

1. 번역의 딜레마: 검증 불가능성

코딩이나 수학은 컴파일러나 정답이라는 명확한 검증 도구(Verifier)가 있어 RLVR(Reinforcement Learning with Verifiable Rewards) 적용이 쉽습니다.

반면 번역은 정답이 모호합니다. 모델이 그럴듯한 오역(Hallucination)을 해도 이를 잡아내 '에너지(패널티)'를 부여할 방법이 마땅치 않았습니다. 그래서 지금까지의 번역 모델은 SFT(지도 학습)에 크게 의존해왔습니다.

2. DuPO의 해법: 역방향 검증 (Back-Translation as Verification)

DuPO는 일반화된 쌍대성을 통해 모델 스스로 모순을 측정하게 만듭니다. 핵심은 입력(x)과 출력(y)의 관계를 역으로 이용하는 것입니다.

 - Forward: 원문(x) → 번역문(y)
 - Backward (Dual): 번역문(y) → 원문 복원(x')

"네가 번역한 문장(y)이 정확하다면, 이를 다시 역번역했을 때 원문(x)의 의미가 보존되어야 한다"는 논리입니다.

논문은 이 복원 과정의 정확도를 보상(Reward)으로 삼아, 외부 정답 데이터(Ground Truth) 없이도 모델이 스스로 정합성을 학습하는 Self-Supervised RL 파이프라인을 구축했습니다.

3. 데이터 효율성과 성과 (Seed-X-7B)

이 방법론을 적용한 Seed-X-7B 모델의 실험 결과는 데이터 효율성 측면에서 시사하는 바가 큽니다.

 - 데이터 효율성: DuPO 강화학습 단계에서 추가로 사용된 데이터는 언어당 약 1,000개의 프롬프트와 검증용(Flores-200) 약 7,000개가 전부였습니다. 수십만 건의 데이터 없이도 모델의 성능을 끌어올렸습니다.
 - 성능 향상: 28개 언어, 756개 번역 방향에서 평균 COMET 점수가 +2.13점 상승했습니다.
 - 비교 우위 (Table 1): 7B 파라미터 크기임에도 GPT-4o, DeepSeek-R1과 동등한 수준의 번역 품질을 기록했으며, Google Translate 대비 유의미한 성능 격차를 보였습니다.

4. 현실적인 트레이드오프: PPO vs DuPO

이 논문은 DuPO의 한계도 명확히 합니다. 사람의 선호도 데이터(Human Preference Data, 약 2만 쌍)를 사용해 학습한 PPO 모델이 DuPO 모델보다 약 0.7% (BLEURT 기준) 더 높은 성능을 보였습니다.

냉정하게 계산하면, DuPO는 PPO가 SFT 대비 끌어올린 성능 향상분의 약 58%~70% 정도를 따라잡았습니다. (BLEURT/COMET 기준)

하지만 관점을 '비용'으로 돌리면 DuPO의 가치가 드러납니다.

 - PPO: 고비용의 Human Labeling Data 필요.
 - DuPO: 정답 없는 Raw Text (Self-Play)만으로 학습 가능.

비용이 거의 들지 않는 방식으로 PPO 성능의 99% 이상을 따라잡았다는 것은, 자원이 제한적인 환경에서 매우 강력한 선택지가 됩니다.


우리는 그동안 성능을 높이기 위해 더 많은 데이터와 더 큰 모델(Scaling Law)에 집중해왔습니다. 하지만 DuPO는 르쿤의 통찰처럼 "스스로 모순을 검증하는(Self-Verify)" 메커니즘을 도입하면, 작은 모델과 적은 데이터로도 SOTA급 모델과 경쟁할 수 있음을 증명합니다.

특히 1.5B 수준의 소형 모델에서도 리랭킹 등을 통해 유의미한 성능 향상(+18.7%p, Math task)을 보인 점은, 로컬 LLM을 연구하는 우리에게 중요한 이정표가 됩니다.

82

love
마음에 쏙듬



