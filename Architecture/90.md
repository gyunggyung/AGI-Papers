---
id: 90
category: Architecture
title: LLM의 "입력 길이 제곱(N^2)"의 저주, 누가 먼저 끊어낼 것인가?
---
LLM의 "입력 길이 제곱(N^2)"의 저주, 누가 먼저 끊어낼 것인가?

xLSTM이나 KAN 같은 새로운 아키텍처들이 주목을 받았었습니다. 하지만 좋은 성능을 보이는 모델들을 뜯어보면, 결국 답은 '트랜스포머의 변형'이거나 '트랜스포머와의 하이브리드'였습니다.

완전히 새로운 구조를 만드는 것보다, 기존 트랜스포머를 하드웨어(GPU/NPU) 극한까지 최적화하고 약점을 보완하는 것이 훨씬 어렵지만 강력하다는 것을 증명하는 3가지 최신 모델을 소개합니다. (모두 2025년 11월에서 12월에 공개된 논문입니다)

이 모델들의 공통된 목표는 입력 길이가 길어질수록 연산량이 기하급수적으로 늘어나는 O(N^2)의 복잡도를 선형(O(N))이나 희소(Sparse) 형태로 바꾸는 것입니다.

1. Liquid AI LFM2: "엣지 디바이스를 위한 하드웨어 맞춤형 다이어트"


- O(N^2) 탈출법: 무거운 어텐션(Attention) 대신 연산 효율이 뛰어난 '게이트 단기 컨볼루션(Gated Short Convolution)'을 메인으로 사용하고, 장기 기억이 꼭 필요한 곳에만 소수의 GQA(Grouped Query Attention)를 섞는 '미니멀 하이브리드' 구조를 택했습니다. 

- 성능:

 - 속도: 1.2B 모델 기준, 동급인 Llama-3.2-1B나 Qwen3-1.7B 대비 CPU에서의 Prefill/Decode 처리량이 2배 이상(2.3~2.8배) 빠릅니다. 

 - 정확도: 단순히 빠르기만 한 것이 아닙니다. 지시 이행 능력을 평가하는 IFEval 벤치마크에서 74.89%를 기록하며, 파라미터가 더 많은 Qwen3-1.7B(73.98%)를 넘어섰습니다. 


2. DeepSeek-V3.2: "어텐션(Attention) 자체를 재발명하다"


- O(N^2) 탈출법: 기존 어텐션이 모든 토큰을 다 보는(N^2) 비효율을 막기 위해, DeepSeek Sparse Attention (DSA)을 도입했습니다. 'Lightning Indexer'를 통해 쿼리와 관련된 Top-k개의 토큰만 선별하여 계산함으로써, 긴 문맥에서도 연산 복잡도를 O(L*k) (k는 L보다 훨씬 작음, 사실상 선형) 수준으로 획기적으로 낮췄습니다. 

- 성능 (Speciale 버전):

 - 이러한 효율성을 바탕으로 사후 학습(RL)에 막대한 자원을 투입했습니다. 그 결과, 추론 특화 모델인 'DeepSeek-V3.2-Speciale'은 코딩 대회(IOI 2025)에서 금메달(Score 492/600), 수학 대회(IMO 2025)에서도 금메달(35/42) 수준을 달성했습니다. 

 - AIME 2025 벤치마크에서는 96.0%의 정답률을 기록하며, GPT-5 High나 Gemini-3.0-Pro와 대등한 수준을 보여줍니다. 


3. NVIDIA Nemotron 3: "Mamba와 MoE의 만남, 100만 토큰의 벽을 넘다"


- O(N^2) 탈출법: 입력 길이에 따라 선형적(O(N))으로 연산량이 증가하는 Mamba-2 레이어를 대거 배치하고, 고비용의 어텐션 레이어는 최소화했습니다. 덕분에 100만 토큰(1M Context)이라는 긴 문맥에서도 성능 저하 없이 매끄럽게 작동합니다. 

- 성능:

 - 처리량: 여기에 MoE(전문가 혼합) 구조를 결합해, Qwen3-30B 동급 모델 대비 3.3배 높은 처리량(Throughput)을 달성했습니다. 

 - 정확도: 긴 문맥 처리를 평가하는 RULER(64K) 벤치마크에서 87.5%를 기록, Qwen3(63.55%)를 압도했습니다. 수학 능력(GSM8K) 또한 92.34%로 최상위권입니다. 


💡 Insight: 무작정 키우는 시대는 끝났다

세 모델 모두 "입력 길이가 길어져도 연산량이 폭발하지 않는 구조"를 만드는 데 집중했습니다.

1. 아키텍처 다이어트: Mamba, Conv, Sparse Attention 등 가벼운 연산을 통해 트랜스포머의 O(N^2) 병목을 깨부쉈습니다.

2. 목적 지향 최적화: 엣지 구동(Liquid), 긴 문맥 처리(NVIDIA), 극한의 추론(DeepSeek) 등 각자의 목적에 맞춰 아키텍처를 깎아냈습니다.

3. 성능과 효율의 양립: 속도만 빠른 경량화 모델이 아니라, 구조적 효율성을 바탕으로 더 많은 데이터를 학습하고 더 깊게 추론하여, 동급 대비 수십 퍼센트 이상의 효율과 성능 향상을 동시에 달성했습니다.

LLM의 시대는 끝나지 않았습니다. 오히려 이제 막 '최적화와 효율화'라는 2막이 열린 것 같습니다.


콘텐츠 자격 보기
44




